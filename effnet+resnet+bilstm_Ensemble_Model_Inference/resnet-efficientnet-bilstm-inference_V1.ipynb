{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315be14c",
   "metadata": {
    "id": "tqGQV5fd0dqa",
    "papermill": {
     "duration": 0.005802,
     "end_time": "2025-04-28T07:40:35.102152",
     "exception": false,
     "start_time": "2025-04-28T07:40:35.096350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble Model (Resnet + EfficientNet + BiLSTM) V1 [Inference]\n",
    "<br>\n",
    "Code inspired by: https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-inference-birdclef-25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ea934",
   "metadata": {
    "id": "vdRP9LaT0xvr",
    "papermill": {
     "duration": 0.004524,
     "end_time": "2025-04-28T07:40:35.111775",
     "exception": false,
     "start_time": "2025-04-28T07:40:35.107251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eaabc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:35.122745Z",
     "iopub.status.busy": "2025-04-28T07:40:35.122370Z",
     "iopub.status.idle": "2025-04-28T07:40:54.372970Z",
     "shell.execute_reply": "2025-04-28T07:40:54.371887Z"
    },
    "id": "tqcetutl0dqb",
    "papermill": {
     "duration": 19.25792,
     "end_time": "2025-04-28T07:40:54.374685",
     "exception": false,
     "start_time": "2025-04-28T07:40:35.116765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import copy                 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171b612",
   "metadata": {
    "id": "RHiJoWnA0zlV",
    "papermill": {
     "duration": 0.004679,
     "end_time": "2025-04-28T07:40:54.384646",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.379967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded82741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.396039Z",
     "iopub.status.busy": "2025-04-28T07:40:54.395691Z",
     "iopub.status.idle": "2025-04-28T07:40:54.402208Z",
     "shell.execute_reply": "2025-04-28T07:40:54.401264Z"
    },
    "id": "eiG_JYA_0dqc",
    "papermill": {
     "duration": 0.014286,
     "end_time": "2025-04-28T07:40:54.403903",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.389617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Paths\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv   = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv     = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_path       = '/kaggle/input/all-model'\n",
    "\n",
    "    # Audio & mel parameters\n",
    "    FS = 32_000; WINDOW_SIZE = 5\n",
    "    N_FFT = 1034; HOP_LENGTH = 64; N_MELS = 136\n",
    "    FMIN = 20; FMAX = 16000\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "\n",
    "\n",
    "    # Model / training options\n",
    "    in_channels   = 1\n",
    "    pretrained    = False          #‑‑ trained‑from‑scratch\n",
    "    dropout_rate  = 0.5\n",
    "    mixup_alpha   = 0.0\n",
    "\n",
    "    # Inference\n",
    "    device   = 'cpu'               # change to 'cuda' if available\n",
    "    batch_size = 32\n",
    "    use_tta    = False; tta_count = 3\n",
    "    threshold  = 0.5\n",
    "\n",
    "    # Debug\n",
    "    debug = False\n",
    "    debug_count = 3\n",
    "\n",
    "cfg = CFG()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26846453",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.415075Z",
     "iopub.status.busy": "2025-04-28T07:40:54.414740Z",
     "iopub.status.idle": "2025-04-28T07:40:54.420962Z",
     "shell.execute_reply": "2025-04-28T07:40:54.420035Z"
    },
    "papermill": {
     "duration": 0.01367,
     "end_time": "2025-04-28T07:40:54.422568",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.408898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if cfg.debug:\n",
    "    import numpy as np\n",
    "    import soundfile as sf\n",
    "    import os\n",
    "    \n",
    "    def generate_dummy_train_soundscapes(output_dir, num_files=5, duration_sec=30, sample_rate=32000):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        for i in range(num_files):\n",
    "            dummy_audio = np.random.randn(duration_sec * sample_rate).astype(np.float32) * 0.01\n",
    "            file_path = os.path.join(output_dir, f\"dummy_{i}.ogg\")\n",
    "            sf.write(file_path, dummy_audio, sample_rate)\n",
    "        print(f\"Generated {num_files} dummy train soundscape files in {output_dir}\")\n",
    "    \n",
    "    # Step 1: Generate dummy training soundscape\n",
    "    generate_dummy_train_soundscapes(\"/kaggle/working/train_soundscapes\")\n",
    "    \n",
    "    # Step 2: Redirect your inference to that folder\n",
    "    cfg.test_soundscapes = \"/kaggle/working/train_soundscapes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b2ce0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.434320Z",
     "iopub.status.busy": "2025-04-28T07:40:54.433433Z",
     "iopub.status.idle": "2025-04-28T07:40:54.460880Z",
     "shell.execute_reply": "2025-04-28T07:40:54.459838Z"
    },
    "id": "gSjWHMLe0dqd",
    "papermill": {
     "duration": 0.035052,
     "end_time": "2025-04-28T07:40:54.462772",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.427720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a99e976",
   "metadata": {
    "id": "KZiQcB2i04F4",
    "papermill": {
     "duration": 0.00474,
     "end_time": "2025-04-28T07:40:54.472567",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.467827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 1 (Resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9006b0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.483788Z",
     "iopub.status.busy": "2025-04-28T07:40:54.483350Z",
     "iopub.status.idle": "2025-04-28T07:40:54.499256Z",
     "shell.execute_reply": "2025-04-28T07:40:54.498454Z"
    },
    "id": "IZYri5Jm0dqd",
    "papermill": {
     "duration": 0.023331,
     "end_time": "2025-04-28T07:40:54.500702",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.477371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ResNet 18\n",
    "class GlobalAttentionPool_resnet(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable attention pooling:\n",
    "    - Projects spatial features into keys and values\n",
    "    - Uses a single learnable query to attend over H×W tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, input_dim))  # learnable query vector\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, input_dim)\n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, input_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Expand the query for the batch\n",
    "        query = self.query.expand(batch_size, -1, -1)  # [batch_size, 1, input_dim]\n",
    "\n",
    "        # Attention\n",
    "        attn_output, _ = self.attention(query=query, key=x, value=x)  # Output shape: (batch_size, 1, input_dim)\n",
    "\n",
    "        return attn_output.squeeze(1)  # Remove sequence dimension → shape (batch_size, input_dim)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes, in_channels=1):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, 3, stride=1, padding=1, bias=False)  # 3×3 stem (no maxpool)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))  # Reduce to 4x4 features first (optional)\n",
    "        self.attention_pool = GlobalAttentionPool_resnet(input_dim=512 * block.expansion)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_planes, planes, s))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(self.conv1(x))\n",
    "        out = F.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.pool(out)              # (B, C, 4, 4)\n",
    "        out = out.flatten(2)              # (B, C, 16)\n",
    "        out = out.transpose(1, 2)          # (B, 16, C)\n",
    "        out = self.attention_pool(out)     # (B, C)\n",
    "        out = self.fc(out)                 # (B, num_classes)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def resnet18_spectrogram(num_classes: int, in_channels: int = 1):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884b009",
   "metadata": {
    "papermill": {
     "duration": 0.004723,
     "end_time": "2025-04-28T07:40:54.510651",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.505928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 2 (EfficientNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ac2f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.521952Z",
     "iopub.status.busy": "2025-04-28T07:40:54.521328Z",
     "iopub.status.idle": "2025-04-28T07:40:54.538044Z",
     "shell.execute_reply": "2025-04-28T07:40:54.537354Z"
    },
    "papermill": {
     "duration": 0.024056,
     "end_time": "2025-04-28T07:40:54.539534",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.515478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EfficientNet B0\n",
    "class GlobalAttentionPoolEffNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable attention pooling:\n",
    "     - projects spatial features to K,V\n",
    "     - uses a single learnable query to attend over H×W tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (in_channels // num_heads) ** -0.5\n",
    "\n",
    "        # learnable query token: (1, 1, C)\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, in_channels))\n",
    "\n",
    "        # projectors for keys & values\n",
    "        self.to_k = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
    "        self.to_v = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        # (B, C, H*W) → (B, H*W, C)\n",
    "        k = self.to_k(x).view(B, C, -1).permute(0, 2, 1)\n",
    "        v = self.to_v(x).view(B, C, -1).permute(0, 2, 1)\n",
    "        # expand query to batch\n",
    "        q = self.query.expand(B, -1, -1)                  # (B, 1, C)\n",
    "\n",
    "        # compute attention scores & aggregate\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale     # (B, 1, H*W)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = attn @ v                                    # (B, 1, C)\n",
    "        return out.squeeze(1)                             # (B, C)\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # load label count\n",
    "        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "        cfg.num_classes = len(taxonomy_df)\n",
    "\n",
    "        # backbone (e.g. EfficientNet, ResNet…)\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2\n",
    "        )\n",
    "\n",
    "        # strip off original head\n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            feat_dim = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            feat_dim = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            feat_dim = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        self.pool = GlobalAttentionPoolEffNet(feat_dim, num_heads=8)\n",
    "\n",
    "        hidden_dim = feat_dim // 2\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(p=getattr(cfg, 'dropout_rate', 0.5)),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, cfg.num_classes)\n",
    "\n",
    "        # mixup config\n",
    "        self.mixup_enabled = getattr(cfg, 'mixup_alpha', 0) > 0\n",
    "        if self.mixup_enabled:\n",
    "            self.mixup_alpha = cfg.mixup_alpha\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            x, ta, tb, lam = self.mixup_data(x, targets)\n",
    "        else:\n",
    "            ta, tb, lam = None, None, None\n",
    "\n",
    "        # extract features\n",
    "        feats = self.backbone(x)\n",
    "        if isinstance(feats, dict):\n",
    "            feats = feats['features']\n",
    "\n",
    "        # if 4D feature map → attention‐pool to 1D\n",
    "        if feats.ndim == 4:\n",
    "            feats = self.pool(feats)\n",
    "\n",
    "        # projection head\n",
    "        feats = self.proj_head(feats)\n",
    "\n",
    "        logits = self.classifier(feats)\n",
    "\n",
    "        # mixup‐aware loss\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            loss = self.mixup_criterion(F.binary_cross_entropy_with_logits,\n",
    "                                        logits, ta, tb, lam)\n",
    "            return logits, loss\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def mixup_data(self, x, targets):\n",
    "        B = x.size(0)\n",
    "        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "        idx = torch.randperm(B, device=x.device)\n",
    "        mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "        return mixed_x, targets, targets[idx], lam\n",
    "\n",
    "    def mixup_criterion(self, criterion, pred, y_a, y_b, lam):\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c197aa",
   "metadata": {
    "papermill": {
     "duration": 0.004598,
     "end_time": "2025-04-28T07:40:54.549161",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.544563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 3 (BiLSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e036ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.560367Z",
     "iopub.status.busy": "2025-04-28T07:40:54.559790Z",
     "iopub.status.idle": "2025-04-28T07:40:54.573546Z",
     "shell.execute_reply": "2025-04-28T07:40:54.572599Z"
    },
    "papermill": {
     "duration": 0.021183,
     "end_time": "2025-04-28T07:40:54.575110",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.553927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GlobalAttentionPoolBiLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable attention pooling:\n",
    "     - projects spatial features to K,V\n",
    "     - uses a single learnable query to attend over H×W tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(input_dim, num_heads, batch_first=True)\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim]\n",
    "        batch_size = x.size(0)\n",
    "        query = self.query.expand(batch_size, -1, -1)  # [batch_size, 1, input_dim]\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, _ = self.attention(query, x, x)  # [batch_size, 1, input_dim]\n",
    "        \n",
    "        # Squeeze the sequence dimension\n",
    "        return attn_output.squeeze(1)  # [batch_size, input_dim]\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # load label count\n",
    "        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "        cfg.num_classes = len(taxonomy_df)\n",
    "        \n",
    "        # Define input feature dimension\n",
    "        self.input_dim = cfg.input_dim  # Add this to your config\n",
    "        \n",
    "        # BiLSTM layers\n",
    "        self.lstm_hidden_size = getattr(cfg, 'lstm_hidden_size', 320)\n",
    "        self.lstm_num_layers = getattr(cfg, 'lstm_num_layers', 3)\n",
    "        self.lstm_dropout = getattr(cfg, 'lstm_dropout', 0.4)\n",
    "        \n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            num_layers=self.lstm_num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=self.lstm_dropout if self.lstm_num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Feature dimension after BiLSTM\n",
    "        bilstm_output_dim = self.lstm_hidden_size * 2  # *2 because bidirectional\n",
    "        self.feat_dim = bilstm_output_dim\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.pool = GlobalAttentionPoolBiLSTM(bilstm_output_dim, num_heads=8)\n",
    "        \n",
    "        # Projection head\n",
    "        hidden_dim = bilstm_output_dim // 2\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(bilstm_output_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(p=getattr(cfg, 'dropout_rate', 0.5)),\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, cfg.num_classes)\n",
    "        \n",
    "        # Mixup config\n",
    "        self.mixup_enabled = getattr(cfg, 'mixup_alpha', 0) > 0\n",
    "        if self.mixup_enabled:\n",
    "            self.mixup_alpha = cfg.mixup_alpha\n",
    "            \n",
    "    def forward(self, x, targets=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # For BiLSTM, x should be [batch_size, sequence_length, features]\n",
    "        # Check if reshaping is needed based on input dimensions\n",
    "        if len(x.shape) == 4:  # [B, C, H, W] format (like a spectrogram)\n",
    "            # Reshape for LSTM: [batch_size, seq_len, features]\n",
    "            # Assuming x is [B, C, H, W], reshape to [B, H, W*C] or similar\n",
    "            # This depends on how your data is structured\n",
    "            x = x.permute(0, 2, 1, 3).contiguous()  # [B, H, C, W]\n",
    "            x = x.view(batch_size, x.size(1), -1)  # [B, H, C*W]\n",
    "        \n",
    "        # Apply BiLSTM\n",
    "        lstm_out, _ = self.bilstm(x)  # [B, seq_len, hidden_size*2]\n",
    "        \n",
    "        # Apply attention pooling\n",
    "        pooled = self.pool(lstm_out)  # [B, hidden_size*2]\n",
    "        \n",
    "        # Projection head\n",
    "        proj = self.proj_head(pooled)\n",
    "        \n",
    "        # Mixup logic if needed\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "            index = torch.randperm(batch_size).to(x.device)\n",
    "            \n",
    "            mixed_proj = lam * proj + (1 - lam) * proj[index, :]\n",
    "            logits = self.classifier(mixed_proj)\n",
    "            \n",
    "            return logits, lam, index\n",
    "        \n",
    "        # Standard forward pass\n",
    "        logits = self.classifier(proj)\n",
    "        \n",
    "        if targets is not None:\n",
    "            return logits, None, None\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487460eb",
   "metadata": {
    "papermill": {
     "duration": 0.004921,
     "end_time": "2025-04-28T07:40:54.585101",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.580180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98974e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.596360Z",
     "iopub.status.busy": "2025-04-28T07:40:54.596006Z",
     "iopub.status.idle": "2025-04-28T07:40:54.602796Z",
     "shell.execute_reply": "2025-04-28T07:40:54.601912Z"
    },
    "papermill": {
     "duration": 0.014426,
     "end_time": "2025-04-28T07:40:54.604321",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.589895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BirdCLEFEnsembleModel(nn.Module):\n",
    "    def __init__(self, models, weights=None):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        n = len(models)\n",
    "        if weights is None:\n",
    "            weights = torch.ones(n) / n\n",
    "        self.register_buffer(\"weights\", torch.as_tensor(weights).view(n, 1, 1))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        outs = [torch.sigmoid(m(x)) for m in self.models]          # list of (B,C)\n",
    "        outs = torch.stack(outs)                                   # (n,B,C)\n",
    "        return (outs * self.weights).sum(0)                        # (B,C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becbc3f9",
   "metadata": {
    "papermill": {
     "duration": 0.004608,
     "end_time": "2025-04-28T07:40:54.614261",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.609653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3593fd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.625464Z",
     "iopub.status.busy": "2025-04-28T07:40:54.625146Z",
     "iopub.status.idle": "2025-04-28T07:40:54.632567Z",
     "shell.execute_reply": "2025-04-28T07:40:54.631612Z"
    },
    "id": "sWQVfqKk0dqe",
    "papermill": {
     "duration": 0.01501,
     "end_time": "2025-04-28T07:40:54.634528",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.619518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "\n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data,\n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)),\n",
    "                          mode='constant')\n",
    "\n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "\n",
    "    # Resize if needed\n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return mel_spec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21b4b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.645977Z",
     "iopub.status.busy": "2025-04-28T07:40:54.645250Z",
     "iopub.status.idle": "2025-04-28T07:40:54.658337Z",
     "shell.execute_reply": "2025-04-28T07:40:54.657298Z"
    },
    "id": "Tma1Adh00dqf",
    "papermill": {
     "duration": 0.020487,
     "end_time": "2025-04-28T07:40:54.659974",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.639487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "\n",
    "    model_dir = Path(cfg.model_path)\n",
    "\n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "\n",
    "    return model_files\n",
    "    \n",
    "def load_model_from_path(path, cfg, num_classes):\n",
    "    ckpt     = torch.load(path, map_location=cfg.device)\n",
    "    path_lc  = str(path).lower()\n",
    "\n",
    "    if \"resnet\" in path_lc:\n",
    "        # your standalone ResNet class\n",
    "        model = resnet18_spectrogram(num_classes, cfg.in_channels)\n",
    "\n",
    "    elif \"efficientnet\" in path_lc:\n",
    "        # for timm-based model, piggy-back off your BirdCLEFModel\n",
    "        tmp_cfg = copy.deepcopy(cfg)\n",
    "        tmp_cfg.model_name   = \"efficientnet_b0\"\n",
    "        tmp_cfg.num_classes  = num_classes    # set this so BirdCLEFModel picks it up\n",
    "        model = BirdCLEFModel(tmp_cfg)\n",
    "    \n",
    "    elif \"bilstm\" in path_lc:\n",
    "        tmp_cfg = copy.deepcopy(cfg)\n",
    "        tmp_cfg.model_name   = \"BiLSTM\"\n",
    "        tmp_cfg.input_dim = 256\n",
    "        tmp_cfg.num_classes = num_classes\n",
    "        model = BiLSTM(tmp_cfg)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot infer backbone type from {path}\")\n",
    "\n",
    "    # load weights (strict=False so missing optimizer keys won’t error)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "    return model.to(cfg.device).eval()\n",
    "\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    paths  = find_model_files(cfg)\n",
    "    models = [load_model_from_path(p, cfg, num_classes) for p in paths]\n",
    "    if not models:\n",
    "        raise RuntimeError(\"No .pth files found in model_path\")\n",
    "    # Example:\n",
    "    weights = [0.4, 0.5, 0.1]  # ResNet = 0.4, EfficientNet = 0.5, BiLSTM = 0.1\n",
    "    return BirdCLEFEnsembleModel(models, weights=weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    predictions = []\n",
    "    row_ids     = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "\n",
    "        for segment_idx in range(total_segments):\n",
    "            start = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end   = start + cfg.FS * cfg.WINDOW_SIZE\n",
    "            seg   = audio_data[start:end]\n",
    "\n",
    "            # build row_id\n",
    "            t_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            row_ids.append(f\"{soundscape_id}_{t_sec}\")\n",
    "\n",
    "            # collect preds (with or without TTA)\n",
    "            preds_per_try = []\n",
    "            n_tries = cfg.tta_count if cfg.use_tta else 1\n",
    "\n",
    "            for t in range(n_tries):\n",
    "                # preprocess + TTA\n",
    "                mel = process_audio_segment(seg, cfg)\n",
    "                if cfg.use_tta:\n",
    "                    mel = apply_tta(mel, t)\n",
    "                x = torch.tensor(mel, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(cfg.device)\n",
    "\n",
    "                # single ensemble forward\n",
    "                with torch.no_grad():\n",
    "                    logits = models(x)              # ensemble returns raw logits\n",
    "                    probs  = torch.sigmoid(logits)  # shape (1, C)\n",
    "                preds_per_try.append(probs.cpu().numpy().squeeze())\n",
    "\n",
    "            # average over TTA (or just take the one if no TTA)\n",
    "            final_preds = np.mean(preds_per_try, axis=0)\n",
    "            predictions.append(final_preds)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "    return row_ids, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d3ee9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.671366Z",
     "iopub.status.busy": "2025-04-28T07:40:54.670719Z",
     "iopub.status.idle": "2025-04-28T07:40:54.679142Z",
     "shell.execute_reply": "2025-04-28T07:40:54.678320Z"
    },
    "papermill": {
     "duration": 0.015586,
     "end_time": "2025-04-28T07:40:54.680534",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.664948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_spectrogram_batched(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment, in batches.\"\"\"\n",
    "    batch_size = 32\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "\n",
    "        mel_batch = []\n",
    "        batch_row_ids = []\n",
    "\n",
    "        for segment_idx in range(total_segments):\n",
    "            start = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end = start + cfg.FS * cfg.WINDOW_SIZE\n",
    "            seg = audio_data[start:end]\n",
    "\n",
    "            mel = process_audio_segment(seg, cfg)\n",
    "            if cfg.use_tta:\n",
    "                mel = apply_tta(mel, tta_idx=0)  # If you want TTA later you can adapt\n",
    "\n",
    "            mel_batch.append(mel)\n",
    "\n",
    "            t_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            batch_row_ids.append(f\"{soundscape_id}_{t_sec}\")\n",
    "\n",
    "            if len(mel_batch) == batch_size or segment_idx == total_segments - 1:\n",
    "                # Convert batch to tensor\n",
    "                x = torch.tensor(np.stack(mel_batch), dtype=torch.float32).unsqueeze(1).to(cfg.device)  # (B,1,H,W)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = models(x)             # (B,C)\n",
    "                    probs  = torch.sigmoid(logits) # (B,C)\n",
    "\n",
    "                preds = probs.cpu().numpy()\n",
    "                predictions.extend(preds)\n",
    "                row_ids.extend(batch_row_ids)\n",
    "\n",
    "                # Reset for next batch\n",
    "                mel_batch = []\n",
    "                batch_row_ids = []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd813f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.691796Z",
     "iopub.status.busy": "2025-04-28T07:40:54.691194Z",
     "iopub.status.idle": "2025-04-28T07:40:54.697325Z",
     "shell.execute_reply": "2025-04-28T07:40:54.696431Z"
    },
    "papermill": {
     "duration": 0.01336,
     "end_time": "2025-04-28T07:40:54.698859",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.685499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference_batched(cfg, models, species_ids):\n",
    "    \"\"\"Run batched inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "\n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "\n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for audio_path in tqdm(test_files):\n",
    "        row_ids, predictions = predict_on_spectrogram_batched(str(audio_path), models, cfg, species_ids)\n",
    "        all_row_ids.extend(row_ids)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    return all_row_ids, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29464c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.710146Z",
     "iopub.status.busy": "2025-04-28T07:40:54.709819Z",
     "iopub.status.idle": "2025-04-28T07:40:54.719219Z",
     "shell.execute_reply": "2025-04-28T07:40:54.718195Z"
    },
    "papermill": {
     "duration": 0.016865,
     "end_time": "2025-04-28T07:40:54.720729",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.703864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_spectrogram_batched_optimized(audio_path, models, cfg, species_ids):\n",
    "    batch_size = 64\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "\n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        precomputed_mels = []\n",
    "        all_row_ids = []\n",
    "\n",
    "        # Precompute all segments\n",
    "        for segment_idx in range(total_segments):\n",
    "            start = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end = start + cfg.FS * cfg.WINDOW_SIZE\n",
    "            seg = audio_data[start:end]\n",
    "            mel = process_audio_segment(seg, cfg)\n",
    "            precomputed_mels.append(mel)\n",
    "\n",
    "            t_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            all_row_ids.append(f\"{soundscape_id}_{t_sec}\")\n",
    "\n",
    "        if len(precomputed_mels) == 0:\n",
    "            print(f\"No valid segments: {audio_path}\")\n",
    "            return [], []\n",
    "\n",
    "        # Now batch predict\n",
    "        for idx in range(0, len(precomputed_mels), batch_size):\n",
    "            batch_mels = precomputed_mels[idx:idx+batch_size]\n",
    "            batch_row_ids = all_row_ids[idx:idx+batch_size]\n",
    "\n",
    "            x = torch.tensor(np.stack(batch_mels), dtype=torch.float32).unsqueeze(1).to(cfg.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = models(x)            # (B, C)\n",
    "                probs = torch.sigmoid(logits) # (B, C)\n",
    "\n",
    "            predictions.append(probs.cpu())\n",
    "            row_ids.extend(batch_row_ids)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return [], []\n",
    "\n",
    "    if len(predictions) == 0:\n",
    "        print(f\"No predictions for {audio_path}\")\n",
    "        return [], []\n",
    "\n",
    "    predictions = torch.cat(predictions, dim=0).numpy()\n",
    "    return row_ids, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bc273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.732159Z",
     "iopub.status.busy": "2025-04-28T07:40:54.731761Z",
     "iopub.status.idle": "2025-04-28T07:40:54.738933Z",
     "shell.execute_reply": "2025-04-28T07:40:54.737834Z"
    },
    "papermill": {
     "duration": 0.014582,
     "end_time": "2025-04-28T07:40:54.740524",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.725942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference_batched_optimized(cfg, models, species_ids):\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "\n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "\n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    if len(test_files) == 0:\n",
    "        print(\"No test files found. Returning dummy empty results.\")\n",
    "        return [], np.zeros((0, len(species_ids)))\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for audio_path in tqdm(test_files):\n",
    "        row_ids, preds = predict_on_spectrogram_batched_optimized(str(audio_path), models, cfg, species_ids)\n",
    "        if len(preds) > 0:\n",
    "            all_row_ids.extend(row_ids)\n",
    "            all_predictions.append(preds)\n",
    "\n",
    "    if len(all_predictions) == 0:\n",
    "        print(\"No valid predictions from any files.\")\n",
    "        return [], np.zeros((0, len(species_ids)))\n",
    "\n",
    "    all_predictions = np.vstack(all_predictions)\n",
    "    return all_row_ids, all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56fd95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.752349Z",
     "iopub.status.busy": "2025-04-28T07:40:54.751513Z",
     "iopub.status.idle": "2025-04-28T07:40:54.760820Z",
     "shell.execute_reply": "2025-04-28T07:40:54.759929Z"
    },
    "id": "eKxIddRI0dqf",
    "papermill": {
     "duration": 0.016756,
     "end_time": "2025-04-28T07:40:54.762154",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.745398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_tta(spec, tta_idx):\n",
    "    \"\"\"Apply test-time augmentation\"\"\"\n",
    "    if tta_idx == 0:\n",
    "        # Original spectrogram\n",
    "        return spec\n",
    "    elif tta_idx == 1:\n",
    "        # Time shift (horizontal flip)\n",
    "        return np.flip(spec, axis=1)\n",
    "    elif tta_idx == 2:\n",
    "        # Frequency shift (vertical flip)\n",
    "        return np.flip(spec, axis=0)\n",
    "    else:\n",
    "        return spec\n",
    "\n",
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "\n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "\n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for audio_path in tqdm(test_files):\n",
    "        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "        all_row_ids.extend(row_ids)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "\n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "\n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d0c8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.773641Z",
     "iopub.status.busy": "2025-04-28T07:40:54.773272Z",
     "iopub.status.idle": "2025-04-28T07:40:54.779792Z",
     "shell.execute_reply": "2025-04-28T07:40:54.778935Z"
    },
    "id": "KzRJSFnP0dqg",
    "papermill": {
     "duration": 0.013875,
     "end_time": "2025-04-28T07:40:54.781149",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.767274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference…\")\n",
    "    print(f\"TTA enabled: {cfg.use_tta} \"\n",
    "          f\"(variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "\n",
    "    # load ONE ensemble module\n",
    "    ensemble = load_models(cfg, num_classes) \n",
    "    print(f\"Loaded {len(ensemble.models)} sub-models\")\n",
    "    print(\"Weights:\", ensemble.weights.cpu().numpy().flatten())\n",
    "    \n",
    "    # run inference ─────────────────────────────────────────\n",
    "    row_ids, predictions = run_inference(cfg, ensemble, species_ids)\n",
    "    # row_ids, predictions = run_inference_batched(cfg, ensemble, species_ids)\n",
    "    # row_ids, predictions = run_inference_batched_optimized(cfg, ensemble, species_ids)\n",
    "\n",
    "\n",
    "\n",
    "    # create Kaggle submission ─────────────────────────────\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "    submission_path = \"submission.csv\"\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"Inference completed in {elapsed:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ea54d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-28T07:40:54.792464Z",
     "iopub.status.busy": "2025-04-28T07:40:54.792114Z",
     "iopub.status.idle": "2025-04-28T07:40:58.721543Z",
     "shell.execute_reply": "2025-04-28T07:40:58.720343Z"
    },
    "id": "-1iGBWpU0dqg",
    "papermill": {
     "duration": 3.936805,
     "end_time": "2025-04-28T07:40:58.723137",
     "exception": false,
     "start_time": "2025-04-28T07:40:54.786332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 11971314,
     "datasetId": 7225109,
     "sourceId": 11520367,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 12057048,
     "datasetId": 7238844,
     "sourceId": 11596463,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11447259,
     "datasetId": 6891568,
     "sourceId": 11060723,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 31.167997,
   "end_time": "2025-04-28T07:41:01.375765",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-28T07:40:30.207768",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1d12dfd31e2047699479c1563bbf7604": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fcc833daf6d34a2b9adccf9155997797",
        "IPY_MODEL_ac26613eb7734ce7aaf558715845dc68",
        "IPY_MODEL_4371682b41c243439b0716728150aed4"
       ],
       "layout": "IPY_MODEL_642577d735de4b4bb62fb9a9bf230161",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4371682b41c243439b0716728150aed4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e6de2d40300c4fb7974496623bcf0463",
       "placeholder": "​",
       "style": "IPY_MODEL_84b369cc724242cda8749b3703839b9a",
       "tabbable": null,
       "tooltip": null,
       "value": " 0/0 [00:00&lt;?, ?it/s]"
      }
     },
     "642577d735de4b4bb62fb9a9bf230161": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68d7863c5b014ba78c8c2f08988b1b8f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "6c70873bff454c37890718dc4ada217f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "84b369cc724242cda8749b3703839b9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "99049ad96e8248ed89587891a23d8c2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ac26613eb7734ce7aaf558715845dc68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_68d7863c5b014ba78c8c2f08988b1b8f",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f9cdfec360cb40169cbec851be54aa1c",
       "tabbable": null,
       "tooltip": null,
       "value": 0
      }
     },
     "e6de2d40300c4fb7974496623bcf0463": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f9cdfec360cb40169cbec851be54aa1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fcc833daf6d34a2b9adccf9155997797": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6c70873bff454c37890718dc4ada217f",
       "placeholder": "​",
       "style": "IPY_MODEL_99049ad96e8248ed89587891a23d8c2f",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
