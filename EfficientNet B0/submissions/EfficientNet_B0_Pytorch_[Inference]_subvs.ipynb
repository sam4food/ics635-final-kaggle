{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 91844,
          "databundleVersionId": 11361821,
          "sourceType": "competition"
        },
        {
          "sourceId": 11520367,
          "sourceType": "datasetVersion",
          "datasetId": 7225109
        },
        {
          "sourceId": 11524118,
          "sourceType": "datasetVersion",
          "datasetId": 7227419
        },
        {
          "sourceId": 11570930,
          "sourceType": "datasetVersion",
          "datasetId": 7254241
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## EfficientNet B0 [Inference] vSubmission\n",
        "Code inspired by: https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-inference-birdclef-25"
      ],
      "metadata": {
        "id": "KL5IPuT1HvbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing"
      ],
      "metadata": {
        "id": "pmbcSQ6PH9Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "import logging\n",
        "import time\n",
        "import math\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-26T02:54:41.598608Z",
          "iopub.execute_input": "2025-04-26T02:54:41.599039Z",
          "iopub.status.idle": "2025-04-26T02:54:41.605337Z",
          "shell.execute_reply.started": "2025-04-26T02:54:41.599007Z",
          "shell.execute_reply": "2025-04-26T02:54:41.604124Z"
        },
        "id": "RN1pINyOHvbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "eHVKqBtiIDiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "\n",
        "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
        "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
        "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
        "    model_path = '/kaggle/input/efficientnet-sub-vs1'\n",
        "\n",
        "    # Audio parameters\n",
        "    FS = 32000\n",
        "    WINDOW_SIZE = 5\n",
        "\n",
        "    # Mel spectrogram parameters\n",
        "    N_FFT = 1024\n",
        "    HOP_LENGTH = 512\n",
        "    N_MELS = 128\n",
        "    FMIN = 50\n",
        "    FMAX = 14000\n",
        "    TARGET_SHAPE = (256, 256)\n",
        "\n",
        "    model_name = 'efficientnet_b0'\n",
        "    in_channels = 1\n",
        "    device = 'cpu'\n",
        "\n",
        "    # Inference parameters\n",
        "    batch_size = 16\n",
        "    use_tta = False\n",
        "    tta_count = 3\n",
        "    threshold = 0.5\n",
        "\n",
        "    use_specific_folds = False  # If False, use all found models\n",
        "    folds = [0, 1]  # Used only if use_specific_folds is True\n",
        "\n",
        "    debug = False\n",
        "    debug_count = 3\n",
        "\n",
        "cfg = CFG()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-26T02:54:41.606853Z",
          "iopub.execute_input": "2025-04-26T02:54:41.607278Z",
          "iopub.status.idle": "2025-04-26T02:54:41.628351Z",
          "shell.execute_reply.started": "2025-04-26T02:54:41.607235Z",
          "shell.execute_reply": "2025-04-26T02:54:41.627129Z"
        },
        "id": "XBNx26lRHvbZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Using device: {cfg.device}\")\n",
        "print(f\"Loading taxonomy data...\")\n",
        "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
        "species_ids = taxonomy_df['primary_label'].tolist()\n",
        "num_classes = len(species_ids)\n",
        "print(f\"Number of classes: {num_classes}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-26T02:54:41.630293Z",
          "iopub.execute_input": "2025-04-26T02:54:41.630665Z",
          "iopub.status.idle": "2025-04-26T02:54:41.657302Z",
          "shell.execute_reply.started": "2025-04-26T02:54:41.630627Z",
          "shell.execute_reply": "2025-04-26T02:54:41.656043Z"
        },
        "id": "z9bsoBaBHvba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "lGZqumVsIFrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalAttentionPool(nn.Module):\n",
        "    def __init__(self, in_channels, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale     = (in_channels // num_heads) ** -0.5\n",
        "        self.query     = nn.Parameter(torch.randn(1, 1, in_channels))\n",
        "        self.to_k      = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
        "        self.to_v      = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        k = self.to_k(x).view(B, C, -1).permute(0, 2, 1)  # (B, H*W, C)\n",
        "        v = self.to_v(x).view(B, C, -1).permute(0, 2, 1)  # (B, H*W, C)\n",
        "        q = self.query.expand(B, -1, -1)                 # (B, 1, C)\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale    # (B, 1, H*W)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        out  = attn @ v                                   # (B, 1, C)\n",
        "        return out.squeeze(1)                             # (B, C)\n",
        "\n",
        "class BirdCLEFModel(nn.Module):\n",
        "    def __init__(self, cfg, num_classes):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        # load num_classes from taxonomy\n",
        "        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
        "        num_classes = len(taxonomy_df)\n",
        "\n",
        "        # backbone\n",
        "        self.backbone = timm.create_model(\n",
        "            cfg.model_name,\n",
        "            pretrained=False,\n",
        "            in_chans=cfg.in_channels,\n",
        "            drop_rate=0.2,\n",
        "            drop_path_rate=0.2\n",
        "        )\n",
        "        # strip original head\n",
        "        if 'efficientnet' in cfg.model_name:\n",
        "            feat_dim = self.backbone.classifier.in_features\n",
        "            self.backbone.classifier = nn.Identity()\n",
        "        elif 'resnet' in cfg.model_name:\n",
        "            feat_dim = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()\n",
        "        else:\n",
        "            feat_dim = self.backbone.get_classifier().in_features\n",
        "            self.backbone.reset_classifier(0, '')\n",
        "\n",
        "        # attention pool + projection head\n",
        "        self.pool = GlobalAttentionPool(feat_dim, num_heads=8)\n",
        "        hidden_dim   = feat_dim // 2\n",
        "        self.proj_head = nn.Sequential(\n",
        "            nn.Linear(feat_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.Dropout(p=getattr(cfg, 'dropout_rate', 0.5)),\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)\n",
        "        if isinstance(feats, dict):\n",
        "            feats = feats['features']\n",
        "        if feats.ndim == 4:\n",
        "            feats = self.pool(feats)\n",
        "        feats = self.proj_head(feats)\n",
        "        logits = self.classifier(feats)\n",
        "        return logits"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-26T02:54:41.658813Z",
          "iopub.execute_input": "2025-04-26T02:54:41.659248Z",
          "iopub.status.idle": "2025-04-26T02:54:41.674187Z",
          "shell.execute_reply.started": "2025-04-26T02:54:41.659205Z",
          "shell.execute_reply": "2025-04-26T02:54:41.672695Z"
        },
        "id": "bXu0BiDMHvba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def audio2melspec(audio_data, cfg):\n",
        "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
        "    if np.isnan(audio_data).any():\n",
        "        mean_signal = np.nanmean(audio_data)\n",
        "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
        "\n",
        "    mel_spec = librosa.feature.melspectrogram(\n",
        "        y=audio_data,\n",
        "        sr=cfg.FS,\n",
        "        n_fft=cfg.N_FFT,\n",
        "        hop_length=cfg.HOP_LENGTH,\n",
        "        n_mels=cfg.N_MELS,\n",
        "        fmin=cfg.FMIN,\n",
        "        fmax=cfg.FMAX,\n",
        "        power=2.0\n",
        "    )\n",
        "\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
        "\n",
        "    return mel_spec_norm\n",
        "\n",
        "def process_audio_segment(audio_data, cfg):\n",
        "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
        "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
        "        audio_data = np.pad(audio_data,\n",
        "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)),\n",
        "                          mode='constant')\n",
        "\n",
        "    mel_spec = audio2melspec(audio_data, cfg)\n",
        "\n",
        "    # Resize if needed\n",
        "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
        "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    return mel_spec.astype(np.float32)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-26T02:54:41.675483Z",
          "iopub.execute_input": "2025-04-26T02:54:41.675916Z",
          "iopub.status.idle": "2025-04-26T02:54:41.699076Z",
          "shell.execute_reply.started": "2025-04-26T02:54:41.675872Z",
          "shell.execute_reply": "2025-04-26T02:54:41.697886Z"
        },
        "id": "u45MYPFlHvba"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def find_model_files(cfg):\n",
        "    \"\"\"\n",
        "    Find all .pth model files in the specified model directory\n",
        "    \"\"\"\n",
        "    model_files = []\n",
        "\n",
        "    model_dir = Path(cfg.model_path)\n",
        "\n",
        "    for path in model_dir.glob('**/*.pth'):\n",
        "        model_files.append(str(path))\n",
        "\n",
        "    return model_files\n",
        "\n",
        "def load_models(cfg, num_classes):\n",
        "    \"\"\"\n",
        "    Load all found model files and prepare them for ensemble\n",
        "    \"\"\"\n",
        "    models = []\n",
        "\n",
        "    model_files = find_model_files(cfg)\n",
        "\n",
        "    if not model_files:\n",
        "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
        "        return models\n",
        "\n",
        "    print(f\"Found a total of {len(model_files)} model files.\")\n",
        "\n",
        "    if cfg.use_specific_folds:\n",
        "        filtered_files = []\n",
        "        for fold in cfg.folds:\n",
        "            fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
        "            filtered_files.extend(fold_files)\n",
        "        model_files = filtered_files\n",
        "        print(f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\")\n",
        "\n",
        "    for model_path in model_files:\n",
        "        try:\n",
        "            print(f\"Loading model: {model_path}\")\n",
        "            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device))\n",
        "\n",
        "            model = BirdCLEFModel(cfg, num_classes)\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            model = model.to(cfg.device)\n",
        "            model.eval()\n",
        "\n",
        "            models.append(model)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {model_path}: {e}\")\n",
        "\n",
        "    return models\n",
        "\n",
        "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
        "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
        "    predictions = []\n",
        "    row_ids = []\n",
        "    soundscape_id = Path(audio_path).stem\n",
        "\n",
        "    try:\n",
        "        print(f\"Processing {soundscape_id}\")\n",
        "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
        "\n",
        "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
        "\n",
        "        for segment_idx in range(total_segments):\n",
        "            start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
        "            end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
        "            segment_audio = audio_data[start_sample:end_sample]\n",
        "\n",
        "            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
        "            row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
        "            row_ids.append(row_id)\n",
        "\n",
        "            if cfg.use_tta:\n",
        "                all_preds = []\n",
        "\n",
        "                for tta_idx in range(cfg.tta_count):\n",
        "                    mel_spec = process_audio_segment(segment_audio, cfg)\n",
        "                    mel_spec = apply_tta(mel_spec, tta_idx)\n",
        "\n",
        "                    mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "                    mel_spec = mel_spec.to(cfg.device)\n",
        "\n",
        "                    if len(models) == 1:\n",
        "                        with torch.no_grad():\n",
        "                            outputs = models[0](mel_spec)\n",
        "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
        "                            all_preds.append(probs)\n",
        "                    else:\n",
        "                        segment_preds = []\n",
        "                        for model in models:\n",
        "                            with torch.no_grad():\n",
        "                                outputs = model(mel_spec)\n",
        "                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
        "                                segment_preds.append(probs)\n",
        "\n",
        "                        avg_preds = np.mean(segment_preds, axis=0)\n",
        "                        all_preds.append(avg_preds)\n",
        "\n",
        "                final_preds = np.mean(all_preds, axis=0)\n",
        "            else:\n",
        "                mel_spec = process_audio_segment(segment_audio, cfg)\n",
        "\n",
        "                mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "                mel_spec = mel_spec.to(cfg.device)\n",
        "\n",
        "                if len(models) == 1:\n",
        "                    with torch.no_grad():\n",
        "                        outputs = models[0](mel_spec)\n",
        "                        final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
        "                else:\n",
        "                    segment_preds = []\n",
        "                    for model in models:\n",
        "                        with torch.no_grad():\n",
        "                            outputs = model(mel_spec)\n",
        "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
        "                            segment_preds.append(probs)\n",
        "\n",
        "                    final_preds = np.mean(segment_preds, axis=0)\n",
        "\n",
        "            predictions.append(final_preds)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {audio_path}: {e}\")\n",
        "\n",
        "    return row_ids, predictions"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-26T02:54:41.701386Z",
          "iopub.execute_input": "2025-04-26T02:54:41.701829Z",
          "iopub.status.idle": "2025-04-26T02:54:41.727589Z",
          "shell.execute_reply.started": "2025-04-26T02:54:41.701781Z",
          "shell.execute_reply": "2025-04-26T02:54:41.726215Z"
        },
        "id": "q4MlCW4qHvbb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_tta(spec, tta_idx):\n",
        "    \"\"\"Apply test-time augmentation\"\"\"\n",
        "    if tta_idx == 0:\n",
        "        # Original spectrogram\n",
        "        return spec\n",
        "    elif tta_idx == 1:\n",
        "        # Time shift (horizontal flip)\n",
        "        return np.flip(spec, axis=1)\n",
        "    elif tta_idx == 2:\n",
        "        # Frequency shift (vertical flip)\n",
        "        return np.flip(spec, axis=0)\n",
        "    else:\n",
        "        return spec\n",
        "\n",
        "def run_inference(cfg, models, species_ids):\n",
        "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
        "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
        "\n",
        "    if cfg.debug:\n",
        "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
        "        test_files = test_files[:cfg.debug_count]\n",
        "\n",
        "    print(f\"Found {len(test_files)} test soundscapes\")\n",
        "\n",
        "    all_row_ids = []\n",
        "    all_predictions = []\n",
        "\n",
        "    for audio_path in tqdm(test_files):\n",
        "        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
        "        all_row_ids.extend(row_ids)\n",
        "        all_predictions.extend(predictions)\n",
        "\n",
        "    return all_row_ids, all_predictions\n",
        "\n",
        "def create_submission(row_ids, predictions, species_ids, cfg):\n",
        "    \"\"\"Create submission dataframe\"\"\"\n",
        "    print(\"Creating submission dataframe...\")\n",
        "\n",
        "    submission_dict = {'row_id': row_ids}\n",
        "\n",
        "    for i, species in enumerate(species_ids):\n",
        "        submission_dict[species] = [pred[i] for pred in predictions]\n",
        "\n",
        "    submission_df = pd.DataFrame(submission_dict)\n",
        "\n",
        "    submission_df.set_index('row_id', inplace=True)\n",
        "\n",
        "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
        "\n",
        "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
        "    if missing_cols:\n",
        "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
        "        for col in missing_cols:\n",
        "            submission_df[col] = 0.0\n",
        "\n",
        "    submission_df = submission_df[sample_sub.columns]\n",
        "\n",
        "    submission_df = submission_df.reset_index()\n",
        "\n",
        "    return submission_df\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-26T02:54:41.72896Z",
          "iopub.execute_input": "2025-04-26T02:54:41.729639Z",
          "iopub.status.idle": "2025-04-26T02:54:41.751287Z",
          "shell.execute_reply.started": "2025-04-26T02:54:41.729594Z",
          "shell.execute_reply": "2025-04-26T02:54:41.749974Z"
        },
        "id": "l02dqXwMHvbb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    start_time = time.time()\n",
        "    print(\"Starting BirdCLEF-2025 inference...\")\n",
        "    print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
        "\n",
        "    models = load_models(cfg, num_classes)\n",
        "\n",
        "    if not models:\n",
        "        print(\"No models found! Please check model paths.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
        "\n",
        "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
        "\n",
        "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
        "\n",
        "    submission_path = 'submission.csv'\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    print(f\"Submission saved to {submission_path}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-26T02:54:41.752423Z",
          "iopub.execute_input": "2025-04-26T02:54:41.752753Z",
          "iopub.status.idle": "2025-04-26T02:54:41.773598Z",
          "shell.execute_reply.started": "2025-04-26T02:54:41.752711Z",
          "shell.execute_reply": "2025-04-26T02:54:41.772437Z"
        },
        "id": "VjRMdeXZHvbb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-26T02:54:41.774733Z",
          "iopub.execute_input": "2025-04-26T02:54:41.775071Z",
          "iopub.status.idle": "2025-04-26T02:54:42.094713Z",
          "shell.execute_reply.started": "2025-04-26T02:54:41.775038Z",
          "shell.execute_reply": "2025-04-26T02:54:42.093324Z"
        },
        "id": "6rnDNqHsHvbc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "QYm7MMvrHvbc"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}