{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae9a4bf1-d61d-4dd9-82b8-830ee31566cf",
   "metadata": {},
   "source": [
    "# EfficientNet B0 [Train] v2 \n",
    "<br>\n",
    "Code inspired by: https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-train-birdclef-25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e82c6-a205-4bcf-9b06-62415da31bb3",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f01f71c-8196-424f-a84c-2713a0517088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import timm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168754c-1088-4e81-becd-670d0624c8c9",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a66c39f5dbbce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, best_auc, path=\"checkpoint.pth\"):\n",
    "    torch.save({\n",
    "        'epoch':       epoch,\n",
    "        'model_state_dict':    model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss':        loss,\n",
    "        'best_auc':    best_auc\n",
    "    }, path)\n",
    "\n",
    "    \n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    ck = torch.load(path, weights_only=False)\n",
    "    model.load_state_dict(ck['model_state_dict'])\n",
    "    optimizer.load_state_dict(ck['optimizer_state_dict'])\n",
    "    start_epoch = ck['epoch'] + 1\n",
    "    loss        = ck['loss']\n",
    "    best_auc    = ck.get('best_auc', 0.0)\n",
    "    return model, optimizer, start_epoch, loss, best_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e0f45-f509-4129-a4df-cc369e604ac8",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84112fcd-d47c-4ab0-ae22-23b0642b6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    seed = 42\n",
    "    debug = False # changed this to false  \n",
    "    apex = False\n",
    "    print_freq = 100\n",
    "    num_workers = 2\n",
    "    \n",
    "    OUTPUT_DIR = '/home/samuelse/koa_scratch/ics635-kaggle-comp/kaggleoutput'\n",
    "\n",
    "    train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "    train_csv = '/home/samuelse/koa_scratch/ics635-kaggle-comp/train.csv'\n",
    "    test_soundscapes = '/home/samuelse/koa_scratch/ics635-kaggle-comp/test_soundscapes'\n",
    "    submission_csv = '/home/samuelse/koa_scratch/ics635-kaggle-comp/sample_submission.csv'\n",
    "    taxonomy_csv = '/home/samuelse/koa_scratch/ics635-kaggle-comp/taxonomy.csv'\n",
    "\n",
    "    spectrogram_npy = '/home/samuelse/koa_scratch/ics635-kaggle-comp/mel-spec/birdclef2025_melspec_5sec_256_256.npy'\n",
    " \n",
    "    model_name = 'efficientnet_b0'  \n",
    "    pretrained = True\n",
    "    in_channels = 1\n",
    "\n",
    "    resume = True\n",
    "    checkpoint_path = \"checkpoint.pth\"\n",
    "    \n",
    "    LOAD_DATA = True  \n",
    "    FS = 32000\n",
    "    TARGET_DURATION = 5.0\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "    \n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 14000\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    epochs = 10  \n",
    "    batch_size = 32  \n",
    "    criterion = 'BCEWithLogitsLoss'\n",
    "\n",
    "    n_fold = 5\n",
    "    selected_folds = [0, 1, 2, 3, 4]   \n",
    "\n",
    "    optimizer = 'AdamW'\n",
    "    lr = 5e-4 \n",
    "    weight_decay = 1e-5\n",
    "  \n",
    "    scheduler = 'CosineAnnealingLR'\n",
    "    min_lr = 1e-6\n",
    "    T_max = epochs\n",
    "\n",
    "    aug_prob = 0.5  \n",
    "    mixup_alpha = 0.5  \n",
    "    \n",
    "    def update_debug_settings(self):\n",
    "        if self.debug:\n",
    "            self.epochs = 2\n",
    "            self.selected_folds = [0]\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b17d22-6a0e-4973-9370-788ac11a07b4",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a138dd7-8cb7-4466-b319-ee251133fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4dd301-be6b-4658-a585-ab710c32fd60",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffae19a0-7630-4f64-9df6-a32fdb79571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_file(audio_path, cfg):\n",
    "    \"\"\"Process a single audio file to get the mel spectrogram\"\"\"\n",
    "    try:\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "\n",
    "        target_samples = int(cfg.TARGET_DURATION * cfg.FS)\n",
    "\n",
    "        if len(audio_data) < target_samples:\n",
    "            n_copy = math.ceil(target_samples / len(audio_data))\n",
    "            if n_copy > 1:\n",
    "                audio_data = np.concatenate([audio_data] * n_copy)\n",
    "\n",
    "        # Extract center 5 seconds\n",
    "        start_idx = max(0, int(len(audio_data) / 2 - target_samples / 2))\n",
    "        end_idx = min(len(audio_data), start_idx + target_samples)\n",
    "        center_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "        if len(center_audio) < target_samples:\n",
    "            center_audio = np.pad(center_audio, \n",
    "                                 (0, target_samples - len(center_audio)), \n",
    "                                 mode='constant')\n",
    "\n",
    "        mel_spec = audio2melspec(center_audio, cfg)\n",
    "        \n",
    "        if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "            mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        return mel_spec.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_spectrograms(df, cfg):\n",
    "    \"\"\"Generate spectrograms from audio files\"\"\"\n",
    "    print(\"Generating mel spectrograms from audio files...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_bird_data = {}\n",
    "    errors = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if cfg.debug and i >= 1000:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            samplename = row['samplename']\n",
    "            filepath = row['filepath']\n",
    "            \n",
    "            mel_spec = process_audio_file(filepath, cfg)\n",
    "            \n",
    "            if mel_spec is not None:\n",
    "                all_bird_data[samplename] = mel_spec\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row.filepath}: {e}\")\n",
    "            errors.append((row.filepath, str(e)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Successfully processed {len(all_bird_data)} files out of {len(df)}\")\n",
    "    print(f\"Failed to process {len(errors)} files\")\n",
    "    \n",
    "    return all_bird_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c3a8ce-dd1c-4e5c-9ada-ba5d6ef435c4",
   "metadata": {},
   "source": [
    "## Dataset Prep and Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02a181d-e23a-42d9-a7b4-b14aa672c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEFDatasetFromNPY(Dataset):\n",
    "    def __init__(self, df, cfg, spectrograms=None, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "\n",
    "        self.spectrograms = spectrograms\n",
    "        \n",
    "        taxonomy_df = pd.read_csv(self.cfg.taxonomy_csv)\n",
    "        self.species_ids = taxonomy_df['primary_label'].tolist()\n",
    "        self.num_classes = len(self.species_ids)\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.species_ids)}\n",
    "\n",
    "        if 'filepath' not in self.df.columns:\n",
    "            self.df['filepath'] = self.cfg.train_datadir + '/' + self.df.filename\n",
    "        \n",
    "        if 'samplename' not in self.df.columns:\n",
    "            self.df['samplename'] = self.df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "\n",
    "        sample_names = set(self.df['samplename'])\n",
    "        if self.spectrograms:\n",
    "            found_samples = sum(1 for name in sample_names if name in self.spectrograms)\n",
    "            print(f\"Found {found_samples} matching spectrograms for {mode} dataset out of {len(self.df)} samples\")\n",
    "        \n",
    "        if cfg.debug:\n",
    "            self.df = self.df.sample(min(1000, len(self.df)), random_state=cfg.seed).reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        samplename = row['samplename']\n",
    "        spec = None\n",
    "\n",
    "        if self.spectrograms and samplename in self.spectrograms:\n",
    "            spec = self.spectrograms[samplename]\n",
    "        elif not self.cfg.LOAD_DATA:\n",
    "            spec = process_audio_file(row['filepath'], self.cfg)\n",
    "\n",
    "        if spec is None:\n",
    "            spec = np.zeros(self.cfg.TARGET_SHAPE, dtype=np.float32)\n",
    "            if self.mode == \"train\":  # Only print warning during training\n",
    "                print(f\"Warning: Spectrogram for {samplename} not found and could not be generated\")\n",
    "\n",
    "        spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        if self.mode == \"train\" and random.random() < self.cfg.aug_prob:\n",
    "            spec = self.apply_spec_augmentations(spec)\n",
    "        \n",
    "        target = self.encode_label(row['primary_label'])\n",
    "        \n",
    "        if 'secondary_labels' in row and row['secondary_labels'] not in [[''], None, np.nan]:\n",
    "            if isinstance(row['secondary_labels'], str):\n",
    "                secondary_labels = eval(row['secondary_labels'])\n",
    "            else:\n",
    "                secondary_labels = row['secondary_labels']\n",
    "            \n",
    "            for label in secondary_labels:\n",
    "                if label in self.label_to_idx:\n",
    "                    target[self.label_to_idx[label]] = 1.0\n",
    "        \n",
    "        return {\n",
    "            'melspec': spec, \n",
    "            'target': torch.tensor(target, dtype=torch.float32),\n",
    "            'filename': row['filename']\n",
    "        }\n",
    "    \n",
    "    def apply_spec_augmentations(self, spec):\n",
    "        \"\"\"Apply augmentations to spectrogram\"\"\"\n",
    "    \n",
    "        # Time masking (horizontal stripes)\n",
    "        if random.random() < 0.5:\n",
    "            num_masks = random.randint(1, 3)\n",
    "            for _ in range(num_masks):\n",
    "                width = random.randint(5, 20)\n",
    "                start = random.randint(0, spec.shape[2] - width)\n",
    "                spec[0, :, start:start+width] = 0\n",
    "        \n",
    "        # Frequency masking (vertical stripes)\n",
    "        if random.random() < 0.5:\n",
    "            num_masks = random.randint(1, 3)\n",
    "            for _ in range(num_masks):\n",
    "                height = random.randint(5, 20)\n",
    "                start = random.randint(0, spec.shape[1] - height)\n",
    "                spec[0, start:start+height, :] = 0\n",
    "        \n",
    "        # Random brightness/contrast\n",
    "        if random.random() < 0.5:\n",
    "            gain = random.uniform(0.8, 1.2)\n",
    "            bias = random.uniform(-0.1, 0.1)\n",
    "            spec = spec * gain + bias\n",
    "            spec = torch.clamp(spec, 0, 1) \n",
    "            \n",
    "        return spec\n",
    "    \n",
    "    def encode_label(self, label):\n",
    "        \"\"\"Encode label to one-hot vector\"\"\"\n",
    "        target = np.zeros(self.num_classes)\n",
    "        if label in self.label_to_idx:\n",
    "            target[self.label_to_idx[label]] = 1.0\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c07eced-f582-4cd4-8c47-e98b0a204d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle different sized spectrograms\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return {}\n",
    "        \n",
    "    result = {key: [] for key in batch[0].keys()}\n",
    "    \n",
    "    for item in batch:\n",
    "        for key, value in item.items():\n",
    "            result[key].append(value)\n",
    "    \n",
    "    for key in result:\n",
    "        if key == 'target' and isinstance(result[key][0], torch.Tensor):\n",
    "            result[key] = torch.stack(result[key])\n",
    "        elif key == 'melspec' and isinstance(result[key][0], torch.Tensor):\n",
    "            shapes = [t.shape for t in result[key]]\n",
    "            if len(set(str(s) for s in shapes)) == 1:\n",
    "                result[key] = torch.stack(result[key])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4995e2-d6ae-4b58-a44f-4cba983e87b5",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d7d82c4-984e-4e72-b4c9-91366490761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAttentionPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable attention pooling:\n",
    "     - projects spatial features to K,V\n",
    "     - uses a single learnable query to attend over H×W tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (in_channels // num_heads) ** -0.5\n",
    "\n",
    "        # learnable query token: (1, 1, C)\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, in_channels))\n",
    "\n",
    "        # projectors for keys & values\n",
    "        self.to_k = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
    "        self.to_v = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        # (B, C, H*W) → (B, H*W, C)\n",
    "        k = self.to_k(x).view(B, C, -1).permute(0, 2, 1)\n",
    "        v = self.to_v(x).view(B, C, -1).permute(0, 2, 1)\n",
    "        # expand query to batch\n",
    "        q = self.query.expand(B, -1, -1)                  # (B, 1, C)\n",
    "\n",
    "        # compute attention scores & aggregate\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale     # (B, 1, H*W)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = attn @ v                                    # (B, 1, C)\n",
    "        return out.squeeze(1)                             # (B, C)\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # load label count\n",
    "        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "        cfg.num_classes = len(taxonomy_df)\n",
    "\n",
    "        # backbone (e.g. EfficientNet, ResNet…)\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2\n",
    "        )\n",
    "\n",
    "        # strip off original head\n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            feat_dim = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            feat_dim = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            feat_dim = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        # === replace avg‐pool with attention‐pool ===\n",
    "        self.pool = GlobalAttentionPool(feat_dim, num_heads=8)\n",
    "\n",
    "        # === added small “bottleneck” head for better representation + regularization ===\n",
    "        hidden_dim = feat_dim // 2\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(p=getattr(cfg, 'dropout_rate', 0.5)),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, cfg.num_classes)\n",
    "\n",
    "        # mixup config\n",
    "        self.mixup_enabled = getattr(cfg, 'mixup_alpha', 0) > 0\n",
    "        if self.mixup_enabled:\n",
    "            self.mixup_alpha = cfg.mixup_alpha\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # optional mixup\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            x, ta, tb, lam = self.mixup_data(x, targets)\n",
    "        else:\n",
    "            ta, tb, lam = None, None, None\n",
    "\n",
    "        # extract features\n",
    "        feats = self.backbone(x)\n",
    "        if isinstance(feats, dict):\n",
    "            feats = feats['features']\n",
    "\n",
    "        # if 4D feature map → attention‐pool to 1D\n",
    "        if feats.ndim == 4:\n",
    "            feats = self.pool(feats)\n",
    "\n",
    "        # projection head\n",
    "        feats = self.proj_head(feats)\n",
    "\n",
    "        logits = self.classifier(feats)\n",
    "\n",
    "        # mixup‐aware loss\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            loss = self.mixup_criterion(F.binary_cross_entropy_with_logits,\n",
    "                                        logits, ta, tb, lam)\n",
    "            return logits, loss\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def mixup_data(self, x, targets):\n",
    "        B = x.size(0)\n",
    "        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "        idx = torch.randperm(B, device=x.device)\n",
    "        mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "        return mixed_x, targets, targets[idx], lam\n",
    "\n",
    "    def mixup_criterion(self, criterion, pred, y_a, y_b, lam):\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d507fa7e-3ee3-431d-8465-4532ee9cf81c",
   "metadata": {},
   "source": [
    "## Training Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c08f2c19-1e76-413e-a75f-9398262c01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, cfg):\n",
    "  \n",
    "    if cfg.optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'AdamW':\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Optimizer {cfg.optimizer} not implemented\")\n",
    "        \n",
    "    return optimizer\n",
    "\n",
    "def get_scheduler(optimizer, cfg):\n",
    "   \n",
    "    if cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg.T_max,\n",
    "            eta_min=cfg.min_lr\n",
    "        )\n",
    "    elif cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=cfg.min_lr,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif cfg.scheduler == 'StepLR':\n",
    "        scheduler = lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=cfg.epochs // 3,\n",
    "            gamma=0.5\n",
    "        )\n",
    "    elif cfg.scheduler == 'OneCycleLR':\n",
    "        scheduler = None  \n",
    "    else:\n",
    "        scheduler = None\n",
    "        \n",
    "    return scheduler\n",
    "\n",
    "def get_criterion(cfg):\n",
    " \n",
    "    if cfg.criterion == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Criterion {cfg.criterion} not implemented\")\n",
    "        \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055083fb-6c74-44f8-8ca6-656cf3cb7124",
   "metadata": {},
   "source": [
    "## Training Loop and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f2514a-4beb-4d45-aeca-ada5f0aac641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs, loss = outputs  \n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        all_outputs.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "   \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            if isinstance(batch['melspec'], list):\n",
    "                batch_outputs = []\n",
    "                batch_losses = []\n",
    "                \n",
    "                for i in range(len(batch['melspec'])):\n",
    "                    inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                    target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                    \n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    batch_outputs.append(output.detach().cpu())\n",
    "                    batch_losses.append(loss.item())\n",
    "                \n",
    "                outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "                loss = np.mean(batch_losses)\n",
    "                targets = batch['target'].numpy()\n",
    "                \n",
    "            else:\n",
    "                inputs = batch['melspec'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                targets = targets.detach().cpu().numpy()\n",
    "            \n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "            losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "def calculate_auc(targets, outputs):\n",
    "  \n",
    "    num_classes = targets.shape[1]\n",
    "    aucs = []\n",
    "    \n",
    "    probs = 1 / (1 + np.exp(-outputs))\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        \n",
    "        if np.sum(targets[:, i]) > 0:\n",
    "            class_auc = roc_auc_score(targets[:, i], probs[:, i])\n",
    "            aucs.append(class_auc)\n",
    "    \n",
    "    return np.mean(aucs) if aucs else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66b9006-3b58-487b-9d0f-36de8be7ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, cfg):\n",
    "    taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "    cfg.num_classes = taxonomy_df.shape[0]\n",
    "\n",
    "    if cfg.debug:\n",
    "        cfg.update_debug_settings()\n",
    "\n",
    "    # load precomputed features or prepare on‑the‑fly\n",
    "    spectrograms = None\n",
    "    if cfg.LOAD_DATA:\n",
    "        try:\n",
    "            spectrograms = np.load(cfg.spectrogram_npy, allow_pickle=True).item()\n",
    "            print(f\"Loaded {len(spectrograms)} pre-computed mel spectrograms\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading NPY: {e}\\nSwitching to on‑the‑fly.\")\n",
    "            cfg.LOAD_DATA = False\n",
    "\n",
    "    if not cfg.LOAD_DATA:\n",
    "        if 'filepath' not in df.columns:\n",
    "            df['filepath'] = cfg.train_datadir + '/' + df.filename\n",
    "        if 'samplename' not in df.columns:\n",
    "            df['samplename'] = df.filename.map(\n",
    "                lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0]\n",
    "            )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    best_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['primary_label'])):\n",
    "        if fold not in cfg.selected_folds:\n",
    "            continue\n",
    "\n",
    "        print(f'\\n{\"=\"*20} Fold {fold} {\"=\"*20}')\n",
    "        train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
    "\n",
    "        # dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            BirdCLEFDatasetFromNPY(train_df, cfg, spectrograms, mode='train'),\n",
    "            batch_size=cfg.batch_size, shuffle=True,\n",
    "            num_workers=cfg.num_workers, pin_memory=True,\n",
    "            collate_fn=collate_fn, drop_last=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            BirdCLEFDatasetFromNPY(val_df, cfg, spectrograms, mode='valid'),\n",
    "            batch_size=cfg.batch_size, shuffle=False,\n",
    "            num_workers=cfg.num_workers, pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        # model / optimizer / criterion / scheduler\n",
    "        model     = BirdCLEFModel(cfg).to(cfg.device)\n",
    "        optimizer = get_optimizer(model, cfg)\n",
    "        criterion = get_criterion(cfg)\n",
    "        if cfg.scheduler == 'OneCycleLR':\n",
    "            scheduler = lr_scheduler.OneCycleLR(\n",
    "                optimizer, max_lr=cfg.lr,\n",
    "                steps_per_epoch=len(train_loader),\n",
    "                epochs=cfg.epochs, pct_start=0.1\n",
    "            )\n",
    "        else:\n",
    "            scheduler = get_scheduler(optimizer, cfg)\n",
    "\n",
    "        # decide checkpoint path\n",
    "        ckpt_path = getattr(cfg, 'checkpoint_path', f\"checkpoint_fold{fold}.pth\")\n",
    "\n",
    "        # resume logic: load epoch, optimizer & optionally best_auc\n",
    "        if cfg.resume and os.path.isfile(ckpt_path):\n",
    "            # make load_checkpoint return best_auc as well:\n",
    "            model, optimizer, start_epoch, _, best_auc = load_checkpoint(model, optimizer, ckpt_path)\n",
    "            print(f\"→ Resumed Fold {fold} from epoch {start_epoch}\")\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            best_auc = 0.0\n",
    "\n",
    "        # if we have no saved best_auc, do an initial validate pass\n",
    "        if start_epoch >= cfg.epochs or best_auc == 0.0:\n",
    "            # run one validation so best_auc isn't zero\n",
    "            _, best_auc = validate(model, val_loader, criterion, cfg.device)\n",
    "            print(f\"→ Starting best AUC = {best_auc:.4f}\")\n",
    "\n",
    "        best_epoch = start_epoch\n",
    "        \n",
    "        for epoch in range(start_epoch, cfg.epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "            train_loss, train_auc = train_one_epoch(\n",
    "                model, train_loader, optimizer, criterion,\n",
    "                cfg.device,\n",
    "                scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None\n",
    "            )\n",
    "            val_loss, val_auc = validate(model, val_loader, criterion, cfg.device)\n",
    "\n",
    "            # step non‑OneCycle schedulers\n",
    "            if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            print(f\"Train L: {train_loss:.4f}, AUC: {train_auc:.4f}\")\n",
    "            print(f\" Val  L: {val_loss:.4f}, AUC: {val_auc:.4f}\")\n",
    "\n",
    "            # save best\n",
    "            if val_auc > best_auc:\n",
    "                best_auc, best_epoch = val_auc, epoch + 1\n",
    "                print(f\"→ New best AUC: {best_auc:.4f} @ epoch {best_epoch}\")\n",
    "                # 1) torch.save your full “best model” bundle\n",
    "                torch.save({\n",
    "                    'model_state_dict':   model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': getattr(scheduler, 'state_dict', lambda: None)(),\n",
    "                    'epoch':               epoch,\n",
    "                    'val_auc':             val_auc,\n",
    "                    'train_auc':           train_auc,\n",
    "                    'cfg':                 cfg\n",
    "                }, f\"model_fold{fold}.pth\")\n",
    "                # 2) save lightweight checkpoint for resume\n",
    "                save_checkpoint(model, optimizer, epoch, val_loss, path=ckpt_path)\n",
    "\n",
    "        best_scores.append(best_auc)\n",
    "        print(f\"Fold {fold} best → AUC {best_auc:.4f} @ epoch {best_epoch}\")\n",
    "\n",
    "        # cleanup\n",
    "        del model, optimizer, scheduler, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    for fold, score in enumerate(best_scores):\n",
    "        print(f\"Fold {cfg.selected_folds[fold]}: {score:.4f}\")\n",
    "    print(f\"Mean AUC: {np.mean(best_scores):.4f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebdfdae6-5a71-4ae4-b730-d1dfca56ef70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training data...\n",
      "\n",
      "Starting training...\n",
      "LOAD_DATA is set to True\n",
      "Using pre-computed mel spectrograms from NPY file\n",
      "Resuming from checkpoint? True\n",
      "Loaded 28564 pre-computed mel spectrograms\n",
      "\n",
      "==================== Fold 0 ====================\n",
      "Found 22851 matching spectrograms for train dataset out of 22851 samples\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n",
      "→ Resumed Fold 0 from epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c9925b858e4132b56d70cb0238901b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Starting best AUC = 0.7804\n",
      "Fold 0 best → AUC 0.7804 @ epoch 10\n",
      "\n",
      "==================== Fold 1 ====================\n",
      "Found 22851 matching spectrograms for train dataset out of 22851 samples\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n",
      "→ Resumed Fold 1 from epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573ff0780e4d44819e149dff49ea9529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Starting best AUC = 0.8159\n",
      "Fold 1 best → AUC 0.8159 @ epoch 10\n",
      "\n",
      "==================== Fold 2 ====================\n",
      "Found 22851 matching spectrograms for train dataset out of 22851 samples\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n",
      "→ Resumed Fold 2 from epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba03aff2ea8464294f379786f9aa0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Starting best AUC = 0.8045\n",
      "Fold 2 best → AUC 0.8045 @ epoch 10\n",
      "\n",
      "==================== Fold 3 ====================\n",
      "Found 22851 matching spectrograms for train dataset out of 22851 samples\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n",
      "→ Resumed Fold 3 from epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da21f3c91db4fa3b430c3fc15372f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Starting best AUC = 0.8157\n",
      "Fold 3 best → AUC 0.8157 @ epoch 10\n",
      "\n",
      "==================== Fold 4 ====================\n",
      "Found 22852 matching spectrograms for train dataset out of 22852 samples\n",
      "Found 5712 matching spectrograms for valid dataset out of 5712 samples\n",
      "→ Resumed Fold 4 from epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b74355e79b483f8de6ea861d3b13a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Starting best AUC = 0.8094\n",
      "Fold 4 best → AUC 0.8094 @ epoch 10\n",
      "\n",
      "============================================================\n",
      "Cross-Validation Results:\n",
      "Fold 0: 0.7804\n",
      "Fold 1: 0.8159\n",
      "Fold 2: 0.8045\n",
      "Fold 3: 0.8157\n",
      "Fold 4: 0.8094\n",
      "Mean AUC: 0.8052\n",
      "============================================================\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    \n",
    "    print(\"\\nLoading training data...\")\n",
    "    train_df = pd.read_csv(cfg.train_csv)\n",
    "    taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(f\"LOAD_DATA is set to {cfg.LOAD_DATA}\")\n",
    "    if cfg.LOAD_DATA:\n",
    "        print(\"Using pre-computed mel spectrograms from NPY file\")\n",
    "    else:\n",
    "        print(\"Will generate spectrograms on-the-fly during training\")\n",
    "        \n",
    "    cfg.resume          = True\n",
    "    cfg.checkpoint_path = \"checkpoint.pth\"\n",
    "    print(f\"Resuming from checkpoint? {cfg.resume}\")\n",
    "    run_training(train_df, cfg)\n",
    "    \n",
    "    print(\"\\nTraining complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
