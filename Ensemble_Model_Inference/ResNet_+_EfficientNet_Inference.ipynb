{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqGQV5fd0dqa"
   },
   "source": [
    "# Ensemble Model (Resnet + EfficientNet) [Inference]\n",
    "<br>\n",
    "Code inspired by: https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-inference-birdclef-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdRP9LaT0xvr"
   },
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T11:19:53.599092Z",
     "iopub.status.busy": "2025-04-24T11:19:53.598775Z",
     "iopub.status.idle": "2025-04-24T11:20:07.933550Z",
     "shell.execute_reply": "2025-04-24T11:20:07.932701Z",
     "shell.execute_reply.started": "2025-04-24T11:19:53.599066Z"
    },
    "id": "tqcetutl0dqb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import copy                 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHiJoWnA0zlV"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T11:20:07.935470Z",
     "iopub.status.busy": "2025-04-24T11:20:07.935134Z",
     "iopub.status.idle": "2025-04-24T11:20:07.942217Z",
     "shell.execute_reply": "2025-04-24T11:20:07.940922Z",
     "shell.execute_reply.started": "2025-04-24T11:20:07.935443Z"
    },
    "id": "eiG_JYA_0dqc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Paths\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv   = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv     = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_path       = '/kaggle/input/all-model'\n",
    "\n",
    "    # Audio & mel parameters\n",
    "    FS = 32_000; WINDOW_SIZE = 5\n",
    "    N_FFT = 1024; HOP_LENGTH = 512; N_MELS = 128\n",
    "    FMIN = 50; FMAX = 14_000\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "\n",
    "    # Model / training options\n",
    "    in_channels   = 1\n",
    "    pretrained    = False          #‑‑ trained‑from‑scratch\n",
    "    dropout_rate  = 0.5\n",
    "    mixup_alpha   = 0.0\n",
    "\n",
    "    # Inference\n",
    "    device   = 'cpu'               # change to 'cuda' if available\n",
    "    batch_size = 16\n",
    "    use_tta    = False; tta_count = 3\n",
    "    threshold  = 0.5\n",
    "\n",
    "    # Debug\n",
    "    debug = False\n",
    "    debug_count = 3\n",
    "\n",
    "cfg = CFG()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T11:20:07.943721Z",
     "iopub.status.busy": "2025-04-24T11:20:07.943297Z",
     "iopub.status.idle": "2025-04-24T11:20:07.993440Z",
     "shell.execute_reply": "2025-04-24T11:20:07.992392Z",
     "shell.execute_reply.started": "2025-04-24T11:20:07.943687Z"
    },
    "id": "gSjWHMLe0dqd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZiQcB2i04F4"
   },
   "source": [
    "## Model 1 (Resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T11:20:07.995303Z",
     "iopub.status.busy": "2025-04-24T11:20:07.995038Z",
     "iopub.status.idle": "2025-04-24T11:20:08.011232Z",
     "shell.execute_reply": "2025-04-24T11:20:08.010219Z",
     "shell.execute_reply.started": "2025-04-24T11:20:07.995281Z"
    },
    "id": "IZYri5Jm0dqd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ResNet 18 structure\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes, in_channels=1):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, 3, stride=1, padding=1, bias=False)  # 3×3 stem (no maxpool)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_planes, planes, s))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.pool(out).view(out.size(0), -1)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "def resnet18_spectrogram(num_classes: int, in_channels: int = 1):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, in_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 (EfficientNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# EfficientNet B0\n",
    "class GlobalAttentionPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable attention pooling:\n",
    "     - projects spatial features to K,V\n",
    "     - uses a single learnable query to attend over H×W tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (in_channels // num_heads) ** -0.5\n",
    "\n",
    "        # learnable query token: (1, 1, C)\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, in_channels))\n",
    "\n",
    "        # projectors for keys & values\n",
    "        self.to_k = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
    "        self.to_v = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        # (B, C, H*W) → (B, H*W, C)\n",
    "        k = self.to_k(x).view(B, C, -1).permute(0, 2, 1)\n",
    "        v = self.to_v(x).view(B, C, -1).permute(0, 2, 1)\n",
    "        # expand query to batch\n",
    "        q = self.query.expand(B, -1, -1)                  # (B, 1, C)\n",
    "\n",
    "        # compute attention scores & aggregate\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale     # (B, 1, H*W)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = attn @ v                                    # (B, 1, C)\n",
    "        return out.squeeze(1)                             # (B, C)\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # load label count\n",
    "        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "        cfg.num_classes = len(taxonomy_df)\n",
    "\n",
    "        # backbone (e.g. EfficientNet, ResNet…)\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2\n",
    "        )\n",
    "\n",
    "        # strip off original head\n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            feat_dim = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            feat_dim = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            feat_dim = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        self.pool = GlobalAttentionPool(feat_dim, num_heads=8)\n",
    "\n",
    "        hidden_dim = feat_dim // 2\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(p=getattr(cfg, 'dropout_rate', 0.5)),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, cfg.num_classes)\n",
    "\n",
    "        # mixup config\n",
    "        self.mixup_enabled = getattr(cfg, 'mixup_alpha', 0) > 0\n",
    "        if self.mixup_enabled:\n",
    "            self.mixup_alpha = cfg.mixup_alpha\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            x, ta, tb, lam = self.mixup_data(x, targets)\n",
    "        else:\n",
    "            ta, tb, lam = None, None, None\n",
    "\n",
    "        # extract features\n",
    "        feats = self.backbone(x)\n",
    "        if isinstance(feats, dict):\n",
    "            feats = feats['features']\n",
    "\n",
    "        # if 4D feature map → attention‐pool to 1D\n",
    "        if feats.ndim == 4:\n",
    "            feats = self.pool(feats)\n",
    "\n",
    "        # projection head\n",
    "        feats = self.proj_head(feats)\n",
    "\n",
    "        logits = self.classifier(feats)\n",
    "\n",
    "        # mixup‐aware loss\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            loss = self.mixup_criterion(F.binary_cross_entropy_with_logits,\n",
    "                                        logits, ta, tb, lam)\n",
    "            return logits, loss\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def mixup_data(self, x, targets):\n",
    "        B = x.size(0)\n",
    "        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "        idx = torch.randperm(B, device=x.device)\n",
    "        mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "        return mixed_x, targets, targets[idx], lam\n",
    "\n",
    "    def mixup_criterion(self, criterion, pred, y_a, y_b, lam):\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T11:14:56.323173Z",
     "iopub.status.busy": "2025-04-24T11:14:56.322849Z",
     "iopub.status.idle": "2025-04-24T11:14:56.346758Z",
     "shell.execute_reply": "2025-04-24T11:14:56.345530Z",
     "shell.execute_reply.started": "2025-04-24T11:14:56.323152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BirdCLEFEnsembleModel(nn.Module):\n",
    "    def __init__(self, models, weights=None):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        n = len(models)\n",
    "        if weights is None:\n",
    "            weights = torch.ones(n) / n\n",
    "        self.register_buffer(\"weights\", torch.as_tensor(weights).view(n, 1, 1))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        outs = [torch.sigmoid(m(x)) for m in self.models]          # list of (B,C)\n",
    "        outs = torch.stack(outs)                                   # (n,B,C)\n",
    "        return (outs * self.weights).sum(0)                        # (B,C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T11:14:56.348163Z",
     "iopub.status.busy": "2025-04-24T11:14:56.347798Z",
     "iopub.status.idle": "2025-04-24T11:14:56.370195Z",
     "shell.execute_reply": "2025-04-24T11:14:56.369096Z",
     "shell.execute_reply.started": "2025-04-24T11:14:56.348132Z"
    },
    "id": "sWQVfqKk0dqe",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "\n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data,\n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)),\n",
    "                          mode='constant')\n",
    "\n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "\n",
    "    # Resize if needed\n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return mel_spec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T11:14:56.371725Z",
     "iopub.status.busy": "2025-04-24T11:14:56.371373Z",
     "iopub.status.idle": "2025-04-24T11:14:56.401127Z",
     "shell.execute_reply": "2025-04-24T11:14:56.399958Z",
     "shell.execute_reply.started": "2025-04-24T11:14:56.371691Z"
    },
    "id": "Tma1Adh00dqf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "\n",
    "    model_dir = Path(cfg.model_path)\n",
    "\n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "\n",
    "    return model_files\n",
    "    \n",
    "def load_model_from_path(path, cfg, num_classes):\n",
    "    ckpt     = torch.load(path, map_location=cfg.device)\n",
    "    path_lc  = str(path).lower()\n",
    "\n",
    "    if \"resnet\" in path_lc:\n",
    "        # your standalone ResNet class\n",
    "        model = resnet18_spectrogram(num_classes, cfg.in_channels)\n",
    "\n",
    "    elif \"efficientnet\" in path_lc:\n",
    "        # for timm-based model, piggy-back off your BirdCLEFModel\n",
    "        tmp_cfg = copy.deepcopy(cfg)\n",
    "        tmp_cfg.model_name   = \"efficientnet_b0\"\n",
    "        tmp_cfg.num_classes  = num_classes    # set this so BirdCLEFModel picks it up\n",
    "        model = BirdCLEFModel(tmp_cfg)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot infer backbone type from {path}\")\n",
    "\n",
    "    # load weights (strict=False so missing optimizer keys won’t error)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "    return model.to(cfg.device).eval()\n",
    "\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    paths  = find_model_files(cfg)\n",
    "    models = [load_model_from_path(p, cfg, num_classes) for p in paths]\n",
    "    if not models:\n",
    "        raise RuntimeError(\"No .pth files found in model_path\")\n",
    "    return BirdCLEFEnsembleModel(models)      # <- single nn.Module\n",
    "\n",
    "\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    predictions = []\n",
    "    row_ids     = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "\n",
    "        for segment_idx in range(total_segments):\n",
    "            start = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end   = start + cfg.FS * cfg.WINDOW_SIZE\n",
    "            seg   = audio_data[start:end]\n",
    "\n",
    "            # build row_id\n",
    "            t_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            row_ids.append(f\"{soundscape_id}_{t_sec}\")\n",
    "\n",
    "            # collect preds (with or without TTA)\n",
    "            preds_per_try = []\n",
    "            n_tries = cfg.tta_count if cfg.use_tta else 1\n",
    "\n",
    "            for t in range(n_tries):\n",
    "                # preprocess + TTA\n",
    "                mel = process_audio_segment(seg, cfg)\n",
    "                if cfg.use_tta:\n",
    "                    mel = apply_tta(mel, t)\n",
    "                x = torch.tensor(mel, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(cfg.device)\n",
    "\n",
    "                # single ensemble forward\n",
    "                with torch.no_grad():\n",
    "                    logits = models(x)              # ensemble returns raw logits\n",
    "                    probs  = torch.sigmoid(logits)  # shape (1, C)\n",
    "                preds_per_try.append(probs.cpu().numpy().squeeze())\n",
    "\n",
    "            # average over TTA (or just take the one if no TTA)\n",
    "            final_preds = np.mean(preds_per_try, axis=0)\n",
    "            predictions.append(final_preds)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "    return row_ids, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T11:14:56.403983Z",
     "iopub.status.busy": "2025-04-24T11:14:56.403448Z",
     "iopub.status.idle": "2025-04-24T11:14:56.427195Z",
     "shell.execute_reply": "2025-04-24T11:14:56.426258Z",
     "shell.execute_reply.started": "2025-04-24T11:14:56.403949Z"
    },
    "id": "eKxIddRI0dqf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_tta(spec, tta_idx):\n",
    "    \"\"\"Apply test-time augmentation\"\"\"\n",
    "    if tta_idx == 0:\n",
    "        # Original spectrogram\n",
    "        return spec\n",
    "    elif tta_idx == 1:\n",
    "        # Time shift (horizontal flip)\n",
    "        return np.flip(spec, axis=1)\n",
    "    elif tta_idx == 2:\n",
    "        # Frequency shift (vertical flip)\n",
    "        return np.flip(spec, axis=0)\n",
    "    else:\n",
    "        return spec\n",
    "\n",
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "\n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "\n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for audio_path in tqdm(test_files):\n",
    "        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "        all_row_ids.extend(row_ids)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "\n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "\n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T11:14:56.428584Z",
     "iopub.status.busy": "2025-04-24T11:14:56.428177Z",
     "iopub.status.idle": "2025-04-24T11:14:56.454293Z",
     "shell.execute_reply": "2025-04-24T11:14:56.453168Z",
     "shell.execute_reply.started": "2025-04-24T11:14:56.428553Z"
    },
    "id": "KzRJSFnP0dqg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference…\")\n",
    "    print(f\"TTA enabled: {cfg.use_tta} \"\n",
    "          f\"(variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "\n",
    "    # load ONE ensemble module\n",
    "    ensemble = load_models(cfg, num_classes) \n",
    "    print(f\"Loaded {len(ensemble.models)} sub-models\")\n",
    "    print(\"Weights:\", ensemble.weights.cpu().numpy().flatten())\n",
    "    \n",
    "    # run inference ─────────────────────────────────────────\n",
    "    row_ids, predictions = run_inference(cfg, ensemble, species_ids)\n",
    "\n",
    "    # create Kaggle submission ─────────────────────────────\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "    submission_path = \"submission.csv\"\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"Inference completed in {elapsed:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T11:14:56.455547Z",
     "iopub.status.busy": "2025-04-24T11:14:56.455209Z",
     "iopub.status.idle": "2025-04-24T11:14:57.121671Z",
     "shell.execute_reply": "2025-04-24T11:14:57.120589Z",
     "shell.execute_reply.started": "2025-04-24T11:14:56.455519Z"
    },
    "id": "-1iGBWpU0dqg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 6891568,
     "sourceId": 11060723,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7225109,
     "sourceId": 11520367,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7227419,
     "sourceId": 11524118,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7236647,
     "sourceId": 11552081,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7238844,
     "sourceId": 11552126,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
