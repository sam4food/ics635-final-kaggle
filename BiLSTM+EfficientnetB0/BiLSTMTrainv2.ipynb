{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721ac380",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "378cd6b0",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f0d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import timm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf27988",
   "metadata": {},
   "source": [
    "# **Checkpointss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12032604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, best_auc, path=\"checkpoint.pth\"):\n",
    "    torch.save({\n",
    "        'epoch':       epoch,\n",
    "        'model_state_dict':    model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss':        loss,\n",
    "        'best_auc':    best_auc\n",
    "    }, path)\n",
    "\n",
    "    \n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    ck = torch.load(path, weights_only=False)\n",
    "    model.load_state_dict(ck['model_state_dict'])\n",
    "    optimizer.load_state_dict(ck['optimizer_state_dict'])\n",
    "    start_epoch = ck['epoch'] + 1\n",
    "    loss        = ck['loss']\n",
    "    best_auc    = ck.get('best_auc', 0.0)\n",
    "    return model, optimizer, start_epoch, loss, best_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8125ba",
   "metadata": {},
   "source": [
    "# **Configs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84112fcd-d47c-4ab0-ae22-23b0642b6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    seed = 42\n",
    "    debug = False # changed this to false  \n",
    "    apex = False\n",
    "    print_freq = 100\n",
    "    num_workers = 2\n",
    "    \n",
    "    OUTPUT_DIR = '/home/huynhw/koa_scratch/kaggle635/kaggleoutput'\n",
    "\n",
    "    train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "    train_csv = '/home/huynhw/koa_scratch/kaggle635/train.csv'\n",
    "    test_soundscapes = '/home/huynhw/koa_scratch/kaggle635/test_soundscapes'\n",
    "    submission_csv = '/home/huynhw/koa_scratch/kaggle635/sample_submission.csv'\n",
    "    taxonomy_csv = '/home/huynhw/koa_scratch/kaggle635/taxonomy.csv'\n",
    "\n",
    "    spectrogram_npy = '/home/huynhw/koa_scratch/kaggle635/birdclef2025_melspec_5sec_256_256.npy'\n",
    " \n",
    "    model_name = 'BiLSTM'  \n",
    "    pretrained = True\n",
    "    in_channels = 1\n",
    "\n",
    "    resume = True\n",
    "    checkpoint_path = \"checkpoint.pth\"\n",
    "    \n",
    "    LOAD_DATA = True  \n",
    "    FS = 32000\n",
    "    TARGET_DURATION = 5.0\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "    \n",
    "    N_FFT = 1280\n",
    "    HOP_LENGTH = 640\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 14000\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    epochs = 10  \n",
    "    batch_size = 16  \n",
    "    criterion = 'BCEWithLogitsLoss'\n",
    "\n",
    "    n_fold = 5\n",
    "    selected_folds = [0, 1, 2, 3, 4]   \n",
    "\n",
    "    optimizer = 'AdamW'\n",
    "    lr = 5e-4 \n",
    "    weight_decay = 1e-5\n",
    "  \n",
    "    scheduler = 'CosineAnnealingLR'\n",
    "    min_lr = 1e-6\n",
    "    T_max = epochs\n",
    "\n",
    "    aug_prob = 0.5  \n",
    "    mixup_alpha = 0.5  \n",
    "    input_dim = 256\n",
    "    \n",
    "    def update_debug_settings(self):\n",
    "        if self.debug:\n",
    "            self.epochs = 2\n",
    "            self.selected_folds = [0]\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dd7620",
   "metadata": {},
   "source": [
    "# **Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a138dd7-8cb7-4466-b319-ee251133fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01466a51",
   "metadata": {},
   "source": [
    "# **Preprocessing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae19a0-7630-4f64-9df6-a32fdb79571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_file(audio_path, cfg):\n",
    "    \"\"\"Process a single audio file to get the mel spectrogram\"\"\"\n",
    "    try:\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "\n",
    "        target_samples = int(cfg.TARGET_DURATION * cfg.FS)\n",
    "\n",
    "        if len(audio_data) < target_samples:\n",
    "            n_copy = math.ceil(target_samples / len(audio_data))\n",
    "            if n_copy > 1:\n",
    "                audio_data = np.concatenate([audio_data] * n_copy)\n",
    "\n",
    "        # Extract center 5 seconds\n",
    "        start_idx = max(0, int(len(audio_data) / 2 - target_samples / 2))\n",
    "        end_idx = min(len(audio_data), start_idx + target_samples)\n",
    "        center_audio = audio_data[start_idx:end_idx]\n",
    "\n",
    "        if len(center_audio) < target_samples:\n",
    "            center_audio = np.pad(center_audio, \n",
    "                                 (0, target_samples - len(center_audio)), \n",
    "                                 mode='constant')\n",
    "\n",
    "        mel_spec = audio2melspec(center_audio, cfg)\n",
    "        \n",
    "        if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "            mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        return mel_spec.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_spectrograms(df, cfg):\n",
    "    \"\"\"Generate spectrograms from audio files\"\"\"\n",
    "    print(\"Generating mel spectrograms from audio files...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    all_bird_data = {}\n",
    "    errors = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        if cfg.debug and i >= 1000:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            samplename = row['samplename']\n",
    "            filepath = row['filepath']\n",
    "            \n",
    "            mel_spec = process_audio_file(filepath, cfg)\n",
    "            \n",
    "            if mel_spec is not None:\n",
    "                all_bird_data[samplename] = mel_spec\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row.filepath}: {e}\")\n",
    "            errors.append((row.filepath, str(e)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Processing completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Successfully processed {len(all_bird_data)} files out of {len(df)}\")\n",
    "    print(f\"Failed to process {len(errors)} files\")\n",
    "    \n",
    "    return all_bird_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a598959",
   "metadata": {},
   "source": [
    "# **Dataset Prep and Data Augmentations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a181d-e23a-42d9-a7b4-b14aa672c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdCLEFDatasetFromNPY(Dataset):\n",
    "    def __init__(self, df, cfg, spectrograms=None, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.cfg = cfg\n",
    "        self.mode = mode\n",
    "\n",
    "        self.spectrograms = spectrograms\n",
    "        \n",
    "        taxonomy_df = pd.read_csv(self.cfg.taxonomy_csv)\n",
    "        self.species_ids = taxonomy_df['primary_label'].tolist()\n",
    "        self.num_classes = len(self.species_ids)\n",
    "        self.label_to_idx = {label: idx for idx, label in enumerate(self.species_ids)}\n",
    "\n",
    "        if 'filepath' not in self.df.columns:\n",
    "            self.df['filepath'] = self.cfg.train_datadir + '/' + self.df.filename\n",
    "        \n",
    "        if 'samplename' not in self.df.columns:\n",
    "            self.df['samplename'] = self.df.filename.map(lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0])\n",
    "\n",
    "        sample_names = set(self.df['samplename'])\n",
    "        if self.spectrograms:\n",
    "            found_samples = sum(1 for name in sample_names if name in self.spectrograms)\n",
    "            print(f\"Found {found_samples} matching spectrograms for {mode} dataset out of {len(self.df)} samples\")\n",
    "        \n",
    "        if cfg.debug:\n",
    "            self.df = self.df.sample(min(1000, len(self.df)), random_state=cfg.seed).reset_index(drop=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        samplename = row['samplename']\n",
    "        spec = None\n",
    "\n",
    "        if self.spectrograms and samplename in self.spectrograms:\n",
    "            spec = self.spectrograms[samplename]\n",
    "        elif not self.cfg.LOAD_DATA:\n",
    "            spec = process_audio_file(row['filepath'], self.cfg)\n",
    "\n",
    "        if spec is None:\n",
    "            spec = np.zeros(self.cfg.TARGET_SHAPE, dtype=np.float32)\n",
    "            if self.mode == \"train\":  # Only print warning during training\n",
    "                print(f\"Warning: Spectrogram for {samplename} not found and could not be generated\")\n",
    "\n",
    "        spec = torch.tensor(spec, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        if self.mode == \"train\" and random.random() < self.cfg.aug_prob:\n",
    "            spec = self.apply_spec_augmentations(spec)\n",
    "        \n",
    "        target = self.encode_label(row['primary_label'])\n",
    "        \n",
    "        if 'secondary_labels' in row and row['secondary_labels'] not in [[''], None, np.nan]:\n",
    "            if isinstance(row['secondary_labels'], str):\n",
    "                secondary_labels = eval(row['secondary_labels'])\n",
    "            else:\n",
    "                secondary_labels = row['secondary_labels']\n",
    "            \n",
    "            for label in secondary_labels:\n",
    "                if label in self.label_to_idx:\n",
    "                    target[self.label_to_idx[label]] = 1.0\n",
    "        \n",
    "        return {\n",
    "            'melspec': spec, \n",
    "            'target': torch.tensor(target, dtype=torch.float32),\n",
    "            'filename': row['filename']\n",
    "        }\n",
    "    \n",
    "    def apply_spec_augmentations(self, spec):\n",
    "        \"\"\"Apply augmentations to spectrogram\"\"\"\n",
    "    \n",
    "        # Time masking (horizontal stripes)\n",
    "        if random.random() < 0.5:\n",
    "            num_masks = random.randint(1, 3)\n",
    "            for _ in range(num_masks):\n",
    "                width = random.randint(5, 20)\n",
    "                start = random.randint(0, spec.shape[2] - width)\n",
    "                spec[0, :, start:start+width] = 0\n",
    "        \n",
    "        # Frequency masking (vertical stripes)\n",
    "        if random.random() < 0.5:\n",
    "            num_masks = random.randint(1, 3)\n",
    "            for _ in range(num_masks):\n",
    "                height = random.randint(5, 20)\n",
    "                start = random.randint(0, spec.shape[1] - height)\n",
    "                spec[0, start:start+height, :] = 0\n",
    "        \n",
    "        # Random brightness/contrast\n",
    "        if random.random() < 0.5:\n",
    "            gain = random.uniform(0.8, 1.2)\n",
    "            bias = random.uniform(-0.1, 0.1)\n",
    "            spec = spec * gain + bias\n",
    "            spec = torch.clamp(spec, 0, 1) \n",
    "            \n",
    "        return spec\n",
    "    \n",
    "    def encode_label(self, label):\n",
    "        \"\"\"Encode label to one-hot vector\"\"\"\n",
    "        target = np.zeros(self.num_classes)\n",
    "        if label in self.label_to_idx:\n",
    "            target[self.label_to_idx[label]] = 1.0\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c07eced-f582-4cd4-8c47-e98b0a204d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle different sized spectrograms\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return {}\n",
    "        \n",
    "    result = {key: [] for key in batch[0].keys()}\n",
    "    \n",
    "    for item in batch:\n",
    "        for key, value in item.items():\n",
    "            result[key].append(value)\n",
    "    \n",
    "    for key in result:\n",
    "        if key == 'target' and isinstance(result[key][0], torch.Tensor):\n",
    "            result[key] = torch.stack(result[key])\n",
    "        elif key == 'melspec' and isinstance(result[key][0], torch.Tensor):\n",
    "            shapes = [t.shape for t in result[key]]\n",
    "            if len(set(str(s) for s in shapes)) == 1:\n",
    "                result[key] = torch.stack(result[key])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f48fd",
   "metadata": {},
   "source": [
    "# **BiLSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d82c4-984e-4e72-b4c9-91366490761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAttentionPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable attention pooling:\n",
    "     - projects spatial features to K,V\n",
    "     - uses a single learnable query to attend over H×W tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(input_dim, num_heads, batch_first=True)\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim]\n",
    "        batch_size = x.size(0)\n",
    "        query = self.query.expand(batch_size, -1, -1)  # [batch_size, 1, input_dim]\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, _ = self.attention(query, x, x)  # [batch_size, 1, input_dim]\n",
    "        \n",
    "        # Squeeze the sequence dimension\n",
    "        return attn_output.squeeze(1)  # [batch_size, input_dim]\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # load label count\n",
    "        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "        cfg.num_classes = len(taxonomy_df)\n",
    "        \n",
    "        # Define input feature dimension\n",
    "        self.input_dim = cfg.input_dim  # Add this to your config\n",
    "        \n",
    "        # BiLSTM layers\n",
    "        self.lstm_hidden_size = getattr(cfg, 'lstm_hidden_size', 640)\n",
    "        self.lstm_num_layers = getattr(cfg, 'lstm_num_layers', 3)\n",
    "        self.lstm_dropout = getattr(cfg, 'lstm_dropout', 0.4)\n",
    "        \n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            num_layers=self.lstm_num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=self.lstm_dropout if self.lstm_num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Feature dimension after BiLSTM\n",
    "        bilstm_output_dim = self.lstm_hidden_size * 2  # *2 because bidirectional\n",
    "        self.feat_dim = bilstm_output_dim\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.pool = GlobalAttentionPool(bilstm_output_dim, num_heads=8)\n",
    "        \n",
    "        # Projection head\n",
    "        hidden_dim = bilstm_output_dim // 2\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(bilstm_output_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(p=getattr(cfg, 'dropout_rate', 0.5)),\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, cfg.num_classes)\n",
    "        \n",
    "        # Mixup config\n",
    "        self.mixup_enabled = getattr(cfg, 'mixup_alpha', 0) > 0\n",
    "        if self.mixup_enabled:\n",
    "            self.mixup_alpha = cfg.mixup_alpha\n",
    "            \n",
    "    def forward(self, x, targets=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # For BiLSTM, x should be [batch_size, sequence_length, features]\n",
    "        # Check if reshaping is needed based on input dimensions\n",
    "        if len(x.shape) == 4:  # [B, C, H, W] format (like a spectrogram)\n",
    "            # Reshape for LSTM: [batch_size, seq_len, features]\n",
    "            # Assuming x is [B, C, H, W], reshape to [B, H, W*C] or similar\n",
    "            # This depends on how your data is structured\n",
    "            x = x.permute(0, 2, 1, 3).contiguous()  # [B, H, C, W]\n",
    "            x = x.view(batch_size, x.size(1), -1)  # [B, H, C*W]\n",
    "        \n",
    "        # Apply BiLSTM\n",
    "        lstm_out, _ = self.bilstm(x)  # [B, seq_len, hidden_size*2]\n",
    "        \n",
    "        # Apply attention pooling\n",
    "        pooled = self.pool(lstm_out)  # [B, hidden_size*2]\n",
    "        \n",
    "        # Projection head\n",
    "        proj = self.proj_head(pooled)\n",
    "        \n",
    "        # Mixup logic if needed\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "            index = torch.randperm(batch_size).to(x.device)\n",
    "            \n",
    "            mixed_proj = lam * proj + (1 - lam) * proj[index, :]\n",
    "            logits = self.classifier(mixed_proj)\n",
    "            \n",
    "            return logits, lam, index\n",
    "        \n",
    "        # Standard forward pass\n",
    "        logits = self.classifier(proj)\n",
    "        \n",
    "        if targets is not None:\n",
    "            return logits, None, None\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d601c788",
   "metadata": {},
   "source": [
    "# **Training Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f2c19-1e76-413e-a75f-9398262c01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, cfg):\n",
    "  \n",
    "    if cfg.optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'AdamW':\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    elif cfg.optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=cfg.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=cfg.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Optimizer {cfg.optimizer} not implemented\")\n",
    "        \n",
    "    return optimizer\n",
    "\n",
    "def get_scheduler(optimizer, cfg):\n",
    "   \n",
    "    if cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=cfg.T_max,\n",
    "            eta_min=cfg.min_lr\n",
    "        )\n",
    "    elif cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=cfg.min_lr,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif cfg.scheduler == 'StepLR':\n",
    "        scheduler = lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=cfg.epochs // 3,\n",
    "            gamma=0.5\n",
    "        )\n",
    "    elif cfg.scheduler == 'OneCycleLR':\n",
    "        scheduler = None  \n",
    "    else:\n",
    "        scheduler = None\n",
    "        \n",
    "    return scheduler\n",
    "\n",
    "def get_criterion(cfg):\n",
    " \n",
    "    if cfg.criterion == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Criterion {cfg.criterion} not implemented\")\n",
    "        \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f8423",
   "metadata": {},
   "source": [
    "# **Training Loop and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f2514a-4beb-4d45-aeca-ada5f0aac641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n",
    "    \n",
    "    for step, batch in pbar:\n",
    "    \n",
    "        if isinstance(batch['melspec'], list):\n",
    "            batch_outputs = []\n",
    "            batch_losses = []\n",
    "            \n",
    "            for i in range(len(batch['melspec'])):\n",
    "                inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                batch_outputs.append(output.detach().cpu())\n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "            loss = np.mean(batch_losses)\n",
    "            targets = batch['target'].numpy()\n",
    "            \n",
    "        else:\n",
    "            inputs = batch['melspec'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs, loss = outputs  \n",
    "            else:\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            targets = targets.detach().cpu().numpy()\n",
    "        \n",
    "        if scheduler is not None and isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "            \n",
    "        all_outputs.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "        losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'train_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "   \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            if isinstance(batch['melspec'], list):\n",
    "                batch_outputs = []\n",
    "                batch_losses = []\n",
    "                \n",
    "                for i in range(len(batch['melspec'])):\n",
    "                    inputs = batch['melspec'][i].unsqueeze(0).to(device)\n",
    "                    target = batch['target'][i].unsqueeze(0).to(device)\n",
    "                    \n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    batch_outputs.append(output.detach().cpu())\n",
    "                    batch_losses.append(loss.item())\n",
    "                \n",
    "                outputs = torch.cat(batch_outputs, dim=0).numpy()\n",
    "                loss = np.mean(batch_losses)\n",
    "                targets = batch['target'].numpy()\n",
    "                \n",
    "            else:\n",
    "                inputs = batch['melspec'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                targets = targets.detach().cpu().numpy()\n",
    "            \n",
    "            all_outputs.append(outputs)\n",
    "            all_targets.append(targets)\n",
    "            losses.append(loss if isinstance(loss, float) else loss.item())\n",
    "    \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    auc = calculate_auc(all_targets, all_outputs)\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_loss, auc\n",
    "\n",
    "def calculate_auc(targets, outputs):\n",
    "  \n",
    "    num_classes = targets.shape[1]\n",
    "    aucs = []\n",
    "    \n",
    "    probs = 1 / (1 + np.exp(-outputs))\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        \n",
    "        if np.sum(targets[:, i]) > 0:\n",
    "            class_auc = roc_auc_score(targets[:, i], probs[:, i])\n",
    "            aucs.append(class_auc)\n",
    "    \n",
    "    return np.mean(aucs) if aucs else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b9006-3b58-487b-9d0f-36de8be7ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(df, cfg):\n",
    "    taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "    cfg.num_classes = taxonomy_df.shape[0]\n",
    "\n",
    "    if cfg.debug:\n",
    "        cfg.update_debug_settings()\n",
    "\n",
    "    # load precomputed features or prepare on‑the‑fly\n",
    "    spectrograms = None\n",
    "    if cfg.LOAD_DATA:\n",
    "        try:\n",
    "            spectrograms = np.load(cfg.spectrogram_npy, allow_pickle=True).item()\n",
    "            print(f\"Loaded {len(spectrograms)} pre-computed mel spectrograms\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading NPY: {e}\\nSwitching to on‑the‑fly.\")\n",
    "            cfg.LOAD_DATA = False\n",
    "\n",
    "    if not cfg.LOAD_DATA:\n",
    "        if 'filepath' not in df.columns:\n",
    "            df['filepath'] = cfg.train_datadir + '/' + df.filename\n",
    "        if 'samplename' not in df.columns:\n",
    "            df['samplename'] = df.filename.map(\n",
    "                lambda x: x.split('/')[0] + '-' + x.split('/')[-1].split('.')[0]\n",
    "            )\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=cfg.n_fold, shuffle=True, random_state=cfg.seed)\n",
    "    best_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df, df['primary_label'])):\n",
    "        if fold not in cfg.selected_folds:\n",
    "            continue\n",
    "\n",
    "        print(f'\\n{\"=\"*20} Fold {fold} {\"=\"*20}')\n",
    "        train_df, val_df = df.iloc[train_idx], df.iloc[val_idx]\n",
    "\n",
    "        # dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            BirdCLEFDatasetFromNPY(train_df, cfg, spectrograms, mode='train'),\n",
    "            batch_size=cfg.batch_size, shuffle=True,\n",
    "            num_workers=cfg.num_workers, pin_memory=True,\n",
    "            collate_fn=collate_fn, drop_last=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            BirdCLEFDatasetFromNPY(val_df, cfg, spectrograms, mode='valid'),\n",
    "            batch_size=cfg.batch_size, shuffle=False,\n",
    "            num_workers=cfg.num_workers, pin_memory=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        # model / optimizer / criterion / scheduler\n",
    "        model     = BiLSTM(cfg).to(cfg.device)\n",
    "        optimizer = get_optimizer(model, cfg)\n",
    "        criterion = get_criterion(cfg)\n",
    "        if cfg.scheduler == 'OneCycleLR':\n",
    "            scheduler = lr_scheduler.OneCycleLR(\n",
    "                optimizer, max_lr=cfg.lr,\n",
    "                steps_per_epoch=len(train_loader),\n",
    "                epochs=cfg.epochs, pct_start=0.1\n",
    "            )\n",
    "        else:\n",
    "            scheduler = get_scheduler(optimizer, cfg)\n",
    "\n",
    "        # decide checkpoint path\n",
    "        ckpt_path = getattr(cfg, 'checkpoint_path', f\"checkpoint_fold{fold}.pth\")\n",
    "\n",
    "        # resume logic: load epoch, optimizer & optionally best_auc\n",
    "        if cfg.resume and os.path.isfile(ckpt_path):\n",
    "            # make load_checkpoint return best_auc as well:\n",
    "            model, optimizer, start_epoch, _, best_auc = load_checkpoint(model, optimizer, ckpt_path)\n",
    "            print(f\"→ Resumed Fold {fold} from epoch {start_epoch}\")\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            best_auc = 0.0\n",
    "\n",
    "        # if we have no saved best_auc, do an initial validate pass\n",
    "        if start_epoch >= cfg.epochs or best_auc == 0.0:\n",
    "            # run one validation so best_auc isn't zero\n",
    "            _, best_auc = validate(model, val_loader, criterion, cfg.device)\n",
    "            print(f\"→ Starting best AUC = {best_auc:.4f}\")\n",
    "\n",
    "        best_epoch = start_epoch\n",
    "        \n",
    "        for epoch in range(start_epoch, cfg.epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{cfg.epochs}\")\n",
    "            train_loss, train_auc = train_one_epoch(\n",
    "                model, train_loader, optimizer, criterion,\n",
    "                cfg.device,\n",
    "                scheduler if isinstance(scheduler, lr_scheduler.OneCycleLR) else None\n",
    "            )\n",
    "            val_loss, val_auc = validate(model, val_loader, criterion, cfg.device)\n",
    "\n",
    "            # step non‑OneCycle schedulers\n",
    "            if scheduler is not None and not isinstance(scheduler, lr_scheduler.OneCycleLR):\n",
    "                if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step(val_loss)\n",
    "                else:\n",
    "                    scheduler.step()\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train AUC: {train_auc:.4f}\")\n",
    "            print(f\" Val  Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "            # save best\n",
    "            if val_auc > best_auc:\n",
    "                best_auc, best_epoch = val_auc, epoch + 1\n",
    "                print(f\"New best AUC: {best_auc:.4f} at epoch {best_epoch}\")\n",
    "                # 1) torch.save your full “best model” bundle\n",
    "                torch.save({\n",
    "                    'model_state_dict':   model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': getattr(scheduler, 'state_dict', lambda: None)(),\n",
    "                    'epoch':               epoch,\n",
    "                    'val_auc':             val_auc,\n",
    "                    'train_auc':           train_auc,\n",
    "                    'cfg':                 cfg\n",
    "                }, f\"model_fold{fold}.pth\")\n",
    "                # 2) save lightweight checkpoint for resume\n",
    "                save_checkpoint(model, optimizer, epoch, val_loss, best_auc, path=ckpt_path)\n",
    "\n",
    "        best_scores.append(best_auc)\n",
    "        print(f\"Fold {fold} best → AUC {best_auc:.4f} @ epoch {best_epoch}\")\n",
    "\n",
    "        # cleanup\n",
    "        del model, optimizer, scheduler, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    for fold, score in enumerate(best_scores):\n",
    "        print(f\"Fold {cfg.selected_folds[fold]}: {score:.4f}\")\n",
    "    print(f\"Mean AUC: {np.mean(best_scores):.4f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdfdae6-5a71-4ae4-b730-d1dfca56ef70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training data...\n",
      "\n",
      "Starting training...\n",
      "LOAD_DATA is set to True\n",
      "Using pre-computed mel spectrograms from NPY file\n",
      "Resuming from checkpoint? True\n",
      "Loaded 28564 pre-computed mel spectrograms\n",
      "\n",
      "==================== Fold 0 ====================\n",
      "Found 22851 matching spectrograms for train dataset out of 22851 samples\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0e473acbfe47aeb1218904d2965c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Starting best AUC = 0.4829\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ce1637469b4ad1b40a15494e1e8099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8412f9ea284cf8af95642334bb7df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0808, Train AUC: 0.5039\n",
      " Val  Loss: 0.0310, Val AUC: 0.5313\n",
      "New best AUC: 0.5313 at epoch 1\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98194b55d2cf420788558e9060d949da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a38bde56acb4a12914cb04359acf163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0315, Train AUC: 0.5145\n",
      " Val  Loss: 0.0310, Val AUC: 0.5424\n",
      "New best AUC: 0.5424 at epoch 2\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09aa21a5a4f44b96a993a70d9f0a229d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11f552ea7b544e3910ca34d4e95e569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1459104899e0>Exception ignored in: \n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x1459104899e0>Traceback (most recent call last):\n",
      "  File \"/home/huynhw/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huynhw/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "  File \"/home/huynhw/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    if w.is_alive():\n",
      "     self._shutdown_workers()\n",
      "  File \"/home/huynhw/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^ ^^^^^\n",
      "  File \"/opt/apps/software/lang/Anaconda3/2024.02-1/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "        ^ ^ ^  ^ ^ ^^  ^^^^^^^^^^^^^^^^\n",
      "^  File \"/opt/apps/software/lang/Anaconda3/2024.02-1/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "  ^ ^ ^ ^ ^^  ^^ ^^^ ^ ^^^^^^^^^\n",
      "^AssertionError^: ^can only test a child process^^\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0313, Train AUC: 0.5094\n",
      " Val  Loss: 0.0308, Val AUC: 0.5435\n",
      "New best AUC: 0.5435 at epoch 3\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ddf18b815a42a79076a1b1a8ba573f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d1ab078d1d4cdf983b1c0de877c473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0311, Train AUC: 0.5094\n",
      " Val  Loss: 0.0308, Val AUC: 0.5577\n",
      "New best AUC: 0.5577 at epoch 4\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b7ddba98ac4243bbe6c6d37c32c261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13afb24f74174207b583d413f1eeea87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0310, Train AUC: 0.5153\n",
      " Val  Loss: 0.0307, Val AUC: 0.5582\n",
      "New best AUC: 0.5582 at epoch 5\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c428a88909ae4bdaa928b1b6632f6bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcd198ebb5a4487bb490daf6f4af430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0310, Train AUC: 0.5225\n",
      " Val  Loss: 0.0307, Val AUC: 0.5612\n",
      "New best AUC: 0.5612 at epoch 6\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4772eb4cb7264019b18f24ba1db00fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc48fa610dc4d4dafbdc2b6ad4cd726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0309, Train AUC: 0.5297\n",
      " Val  Loss: 0.0306, Val AUC: 0.5811\n",
      "New best AUC: 0.5811 at epoch 7\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d336dfdf923349d1851cc995c9035028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98327585ad74431975bb962dea60460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0309, Train AUC: 0.5448\n",
      " Val  Loss: 0.0306, Val AUC: 0.5707\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2492248bb4f94242aaeb5431e6236557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fb42faf7a2048db98f68f3d4f24bb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1459104899e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huynhw/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/huynhw/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/opt/apps/software/lang/Anaconda3/2024.02-1/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0308, Train AUC: 0.5476\n",
      " Val  Loss: 0.0306, Val AUC: 0.5681\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46a43e5ebad450fb56c3a5793f70393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c50b5457ecf48c9ac198c426fa1a091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0308, Train AUC: 0.5427\n",
      " Val  Loss: 0.0306, Val AUC: 0.5659\n",
      "Fold 0 best → AUC 0.5811 @ epoch 7\n",
      "\n",
      "==================== Fold 1 ====================\n",
      "Found 22851 matching spectrograms for train dataset out of 22851 samples\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n",
      "→ Resumed Fold 1 from epoch 7\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea47818e1d24423aa420811f2f54162a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9158dff30cc4206bc05b87e1940ae16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0309, Train AUC: 0.5383\n",
      " Val  Loss: 0.0304, Val AUC: 0.5845\n",
      "New best AUC: 0.5845 at epoch 8\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298119677212440fbdf1dfa3ee2f3a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443955536a70483aaf0214e6608d8df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0309, Train AUC: 0.5318\n",
      " Val  Loss: 0.0304, Val AUC: 0.5869\n",
      "New best AUC: 0.5869 at epoch 9\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a29d81095c541ff8326c69f9e42b6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a2dfe15e7f4e0ba3a3868e21540649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0308, Train AUC: 0.5368\n",
      " Val  Loss: 0.0304, Val AUC: 0.5941\n",
      "New best AUC: 0.5941 at epoch 10\n",
      "Fold 1 best → AUC 0.5941 @ epoch 10\n",
      "\n",
      "==================== Fold 2 ====================\n",
      "Found 22851 matching spectrograms for train dataset out of 22851 samples\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n",
      "→ Resumed Fold 2 from epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43bb30deec1495da6953608414ab526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Starting best AUC = 0.6023\n",
      "Fold 2 best → AUC 0.6023 @ epoch 10\n",
      "\n",
      "==================== Fold 3 ====================\n",
      "Found 22851 matching spectrograms for train dataset out of 22851 samples\n",
      "Found 5713 matching spectrograms for valid dataset out of 5713 samples\n",
      "→ Resumed Fold 3 from epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c60cc37ee624eeab8605e13c20ab6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Starting best AUC = 0.6097\n",
      "Fold 3 best → AUC 0.6097 @ epoch 10\n",
      "\n",
      "==================== Fold 4 ====================\n",
      "Found 22852 matching spectrograms for train dataset out of 22852 samples\n",
      "Found 5712 matching spectrograms for valid dataset out of 5712 samples\n",
      "→ Resumed Fold 4 from epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b755199cade48268a98e23af05c1bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Starting best AUC = 0.5957\n",
      "Fold 4 best → AUC 0.5957 @ epoch 10\n",
      "\n",
      "============================================================\n",
      "Cross-Validation Results:\n",
      "Fold 0: 0.5811\n",
      "Fold 1: 0.5941\n",
      "Fold 2: 0.6023\n",
      "Fold 3: 0.6097\n",
      "Fold 4: 0.5957\n",
      "Mean AUC: 0.5966\n",
      "============================================================\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    \n",
    "    print(\"\\nLoading training data...\")\n",
    "    train_df = pd.read_csv(cfg.train_csv)\n",
    "    taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(f\"LOAD_DATA is set to {cfg.LOAD_DATA}\")\n",
    "    if cfg.LOAD_DATA:\n",
    "        print(\"Using pre-computed mel spectrograms from NPY file\")\n",
    "    else:\n",
    "        print(\"Will generate spectrograms on-the-fly during training\")\n",
    "        \n",
    "    cfg.resume          = True\n",
    "    cfg.checkpoint_path = \"checkpoint.pth\"\n",
    "    print(f\"Resuming from checkpoint? {cfg.resume}\")\n",
    "    run_training(train_df, cfg)\n",
    "    \n",
    "    print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b4cbe-2225-4246-9d2a-8ae6d2b1a655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
