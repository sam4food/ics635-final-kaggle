{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BiLSTM + EfficientnetB0 Inference Notebook**\n",
    " Code inspired by: https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-train-birdclef-25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T06:46:02.131180Z",
     "iopub.status.busy": "2025-04-26T06:46:02.130791Z",
     "iopub.status.idle": "2025-04-26T06:46:02.137324Z",
     "shell.execute_reply": "2025-04-26T06:46:02.136015Z",
     "shell.execute_reply.started": "2025-04-26T06:46:02.131151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import copy                 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Configs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T06:46:02.138850Z",
     "iopub.status.busy": "2025-04-26T06:46:02.138569Z",
     "iopub.status.idle": "2025-04-26T06:46:02.153596Z",
     "shell.execute_reply": "2025-04-26T06:46:02.152500Z",
     "shell.execute_reply.started": "2025-04-26T06:46:02.138825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    " \n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_path = '/kaggle/input/best-test'  \n",
    "    \n",
    "    # Audio & mel parameters\n",
    "    FS = 32_000; WINDOW_SIZE = 5\n",
    "    N_FFT = 1024; HOP_LENGTH = 512; N_MELS = 128\n",
    "    FMIN = 50; FMAX = 14_000\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "\n",
    "    # Model / training options\n",
    "    in_channels   = 1\n",
    "    pretrained    = False          #‑‑ trained‑from‑scratch\n",
    "    dropout_rate  = 0.5\n",
    "    mixup_alpha   = 0.0\n",
    "\n",
    "    # Inference\n",
    "    device   = 'cpu'               # change to 'cuda' if available\n",
    "    batch_size = 16\n",
    "    use_tta    = False; tta_count = 3\n",
    "    threshold  = 0.5\n",
    "\n",
    "    # Debug\n",
    "    debug = False\n",
    "    debug_count = 3\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T06:46:02.155433Z",
     "iopub.status.busy": "2025-04-26T06:46:02.155083Z",
     "iopub.status.idle": "2025-04-26T06:46:02.182134Z",
     "shell.execute_reply": "2025-04-26T06:46:02.180908Z",
     "shell.execute_reply.started": "2025-04-26T06:46:02.155407Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model 1 BiLSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T06:46:02.184223Z",
     "iopub.status.busy": "2025-04-26T06:46:02.183730Z",
     "iopub.status.idle": "2025-04-26T06:46:02.199193Z",
     "shell.execute_reply": "2025-04-26T06:46:02.198147Z",
     "shell.execute_reply.started": "2025-04-26T06:46:02.184180Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GlobalAttentionPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable attention pooling:\n",
    "     - projects spatial features to K,V\n",
    "     - uses a single learnable query to attend over H×W tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(input_dim, num_heads, batch_first=True)\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, input_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim]\n",
    "        batch_size = x.size(0)\n",
    "        query = self.query.expand(batch_size, -1, -1)  # [batch_size, 1, input_dim]\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, _ = self.attention(query, x, x)  # [batch_size, 1, input_dim]\n",
    "        \n",
    "        # Squeeze the sequence dimension\n",
    "        return attn_output.squeeze(1)  # [batch_size, input_dim]\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, cfg, num_classes=None):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "    \n",
    "        # Use either passed num_classes or get from taxonomy\n",
    "        if num_classes is not None:\n",
    "            cfg.num_classes = num_classes\n",
    "        else:\n",
    "        # load label count from taxonomy file\n",
    "            taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "            cfg.num_classes = len(taxonomy_df)\n",
    "        \n",
    "        # Define input feature dimension\n",
    "        self.input_dim = cfg.input_dim  # Add this to your config\n",
    "        \n",
    "        # BiLSTM layers\n",
    "        self.lstm_hidden_size = getattr(cfg, 'lstm_hidden_size', 256)\n",
    "        self.lstm_num_layers = getattr(cfg, 'lstm_num_layers', 2)\n",
    "        self.lstm_dropout = getattr(cfg, 'lstm_dropout', 0.2)\n",
    "        \n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=self.input_dim,\n",
    "            hidden_size=self.lstm_hidden_size,\n",
    "            num_layers=self.lstm_num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=self.lstm_dropout if self.lstm_num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Feature dimension after BiLSTM\n",
    "        bilstm_output_dim = self.lstm_hidden_size * 2  # *2 because bidirectional\n",
    "        self.feat_dim = bilstm_output_dim\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.pool = GlobalAttentionPool(bilstm_output_dim, num_heads=8)\n",
    "        \n",
    "        # Projection head\n",
    "        hidden_dim = bilstm_output_dim // 2\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(bilstm_output_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(p=getattr(cfg, 'dropout_rate', 0.5)),\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, cfg.num_classes)\n",
    "        \n",
    "        # Mixup config\n",
    "        self.mixup_enabled = getattr(cfg, 'mixup_alpha', 0) > 0\n",
    "        if self.mixup_enabled:\n",
    "            self.mixup_alpha = cfg.mixup_alpha\n",
    "            \n",
    "    def forward(self, x, targets=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # For BiLSTM, x should be [batch_size, sequence_length, features]\n",
    "        # Check if reshaping is needed based on input dimensions\n",
    "        if len(x.shape) == 4:  # [B, C, H, W] format (like a spectrogram)\n",
    "            # Reshape for LSTM: [batch_size, seq_len, features]\n",
    "            # Assuming x is [B, C, H, W], reshape to [B, H, W*C] or similar\n",
    "            # This depends on how your data is structured\n",
    "            x = x.permute(0, 2, 1, 3).contiguous()  # [B, H, C, W]\n",
    "            x = x.view(batch_size, x.size(1), -1)  # [B, H, C*W]\n",
    "        \n",
    "        # Apply BiLSTM\n",
    "        lstm_out, _ = self.bilstm(x)  # [B, seq_len, hidden_size*2]\n",
    "        \n",
    "        # Apply attention pooling\n",
    "        pooled = self.pool(lstm_out)  # [B, hidden_size*2]\n",
    "        \n",
    "        # Projection head\n",
    "        proj = self.proj_head(pooled)\n",
    "        \n",
    "        # Mixup logic if needed\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "            index = torch.randperm(batch_size).to(x.device)\n",
    "            \n",
    "            mixed_proj = lam * proj + (1 - lam) * proj[index, :]\n",
    "            logits = self.classifier(mixed_proj)\n",
    "            \n",
    "            return logits, lam, index\n",
    "        \n",
    "        # Standard forward pass\n",
    "        logits = self.classifier(proj)\n",
    "        \n",
    "        if targets is not None:\n",
    "            return logits, None, None\n",
    "        else:\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model 2 Efficientnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T06:46:02.200921Z",
     "iopub.status.busy": "2025-04-26T06:46:02.200508Z",
     "iopub.status.idle": "2025-04-26T06:46:02.226175Z",
     "shell.execute_reply": "2025-04-26T06:46:02.225093Z",
     "shell.execute_reply.started": "2025-04-26T06:46:02.200884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# EfficientNet B0\n",
    "class GlobalAttentionPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable attention pooling:\n",
    "     - projects spatial features to K,V\n",
    "     - uses a single learnable query to attend over H×W tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (in_channels // num_heads) ** -0.5\n",
    "\n",
    "        # learnable query token: (1, 1, C)\n",
    "        self.query = nn.Parameter(torch.randn(1, 1, in_channels))\n",
    "\n",
    "        # projectors for keys & values\n",
    "        self.to_k = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
    "        self.to_v = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        B, C, H, W = x.shape\n",
    "        # (B, C, H*W) → (B, H*W, C)\n",
    "        k = self.to_k(x).view(B, C, -1).permute(0, 2, 1)\n",
    "        v = self.to_v(x).view(B, C, -1).permute(0, 2, 1)\n",
    "        # expand query to batch\n",
    "        q = self.query.expand(B, -1, -1)                  # (B, 1, C)\n",
    "\n",
    "        # compute attention scores & aggregate\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale     # (B, 1, H*W)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = attn @ v                                    # (B, 1, C)\n",
    "        return out.squeeze(1)                             # (B, C)\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # load label count\n",
    "        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "        cfg.num_classes = len(taxonomy_df)\n",
    "\n",
    "        # backbone (e.g. EfficientNet, ResNet…)\n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2\n",
    "        )\n",
    "\n",
    "        # strip off original head\n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            feat_dim = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            feat_dim = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            feat_dim = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "\n",
    "        self.pool = GlobalAttentionPool(feat_dim, num_heads=8)\n",
    "\n",
    "        hidden_dim = feat_dim // 2\n",
    "        self.proj_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(p=getattr(cfg, 'dropout_rate', 0.5)),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, cfg.num_classes)\n",
    "\n",
    "        # mixup config\n",
    "        self.mixup_enabled = getattr(cfg, 'mixup_alpha', 0) > 0\n",
    "        if self.mixup_enabled:\n",
    "            self.mixup_alpha = cfg.mixup_alpha\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            x, ta, tb, lam = self.mixup_data(x, targets)\n",
    "        else:\n",
    "            ta, tb, lam = None, None, None\n",
    "\n",
    "        # extract features\n",
    "        feats = self.backbone(x)\n",
    "        if isinstance(feats, dict):\n",
    "            feats = feats['features']\n",
    "\n",
    "        # if 4D feature map → attention‐pool to 1D\n",
    "        if feats.ndim == 4:\n",
    "            feats = self.pool(feats)\n",
    "\n",
    "        # projection head\n",
    "        feats = self.proj_head(feats)\n",
    "\n",
    "        logits = self.classifier(feats)\n",
    "\n",
    "        # mixup‐aware loss\n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            loss = self.mixup_criterion(F.binary_cross_entropy_with_logits,\n",
    "                                        logits, ta, tb, lam)\n",
    "            return logits, loss\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def mixup_data(self, x, targets):\n",
    "        B = x.size(0)\n",
    "        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
    "        idx = torch.randperm(B, device=x.device)\n",
    "        mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "        return mixed_x, targets, targets[idx], lam\n",
    "\n",
    "    def mixup_criterion(self, criterion, pred, y_a, y_b, lam):\n",
    "        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **Ensembel Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T06:46:02.227523Z",
     "iopub.status.busy": "2025-04-26T06:46:02.227248Z",
     "iopub.status.idle": "2025-04-26T06:46:02.250256Z",
     "shell.execute_reply": "2025-04-26T06:46:02.249112Z",
     "shell.execute_reply.started": "2025-04-26T06:46:02.227499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BirdCLEFEnsembleModel(nn.Module):\n",
    "    def __init__(self, models, weights=None):\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList(models)\n",
    "        n = len(models)\n",
    "        if weights is None:\n",
    "            weights = torch.ones(n) / n\n",
    "        self.register_buffer(\"weights\", torch.as_tensor(weights).view(n, 1, 1))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        outs = [torch.sigmoid(m(x)) for m in self.models]          # list of (B,C)\n",
    "        outs = torch.stack(outs)                                   # (n,B,C)\n",
    "        return (outs * self.weights).sum(0)                        # (B,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utilities and Tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T06:46:02.252679Z",
     "iopub.status.busy": "2025-04-26T06:46:02.252387Z",
     "iopub.status.idle": "2025-04-26T06:46:02.271381Z",
     "shell.execute_reply": "2025-04-26T06:46:02.269975Z",
     "shell.execute_reply.started": "2025-04-26T06:46:02.252655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data, \n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "                          mode='constant')\n",
    "    \n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    return mel_spec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T06:46:02.272959Z",
     "iopub.status.busy": "2025-04-26T06:46:02.272683Z",
     "iopub.status.idle": "2025-04-26T06:46:02.287188Z",
     "shell.execute_reply": "2025-04-26T06:46:02.286048Z",
     "shell.execute_reply.started": "2025-04-26T06:46:02.272935Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "\n",
    "    model_dir = Path(cfg.model_path)\n",
    "\n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "\n",
    "    return model_files\n",
    "    \n",
    "def load_model_from_path(path, cfg, num_classes):\n",
    "    ckpt = torch.load(path, map_location=cfg.device)\n",
    "    path_lc = str(path).lower()\n",
    "    \n",
    "    # Add support for your specific model name pattern\n",
    "    if \"bilstm\" in path_lc:\n",
    "        tmp_cfg = copy.deepcopy(cfg)\n",
    "        tmp_cfg.num_classes = num_classes\n",
    "        model = BiLSTM(tmp_cfg)\n",
    "    elif \"efficientnet\" in path_lc:\n",
    "        tmp_cfg = copy.deepcopy(cfg)\n",
    "        tmp_cfg.model_name = \"efficientnet_b0\"\n",
    "        tmp_cfg.num_classes = num_classes\n",
    "        model = BirdCLEFModel(tmp_cfg)\n",
    "    elif \"fold\" in path_lc:  # Add specific check for \"fold\" in the filename\n",
    "        # Assuming the fold models are EfficientNet models\n",
    "        tmp_cfg = copy.deepcopy(cfg)\n",
    "        tmp_cfg.model_name = \"efficientnet_b0\"\n",
    "        tmp_cfg.num_classes = num_classes\n",
    "        model = BirdCLEFModel(tmp_cfg)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot infer backbone type from {path}\")\n",
    "    \n",
    "    # load weights (strict=False so missing optimizer keys won't error)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "    return model.to(cfg.device).eval()\n",
    "\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    paths  = find_model_files(cfg)\n",
    "    models = [load_model_from_path(p, cfg, num_classes) for p in paths]\n",
    "    if not models:\n",
    "        raise RuntimeError(\"No .pth files found in model_path\")\n",
    "    return BirdCLEFEnsembleModel(models)      # <- single nn.Module\n",
    "\n",
    "\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    predictions = []\n",
    "    row_ids     = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "\n",
    "        for segment_idx in range(total_segments):\n",
    "            start = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end   = start + cfg.FS * cfg.WINDOW_SIZE\n",
    "            seg   = audio_data[start:end]\n",
    "\n",
    "            # build row_id\n",
    "            t_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            row_ids.append(f\"{soundscape_id}_{t_sec}\")\n",
    "\n",
    "            # collect preds (with or without TTA)\n",
    "            preds_per_try = []\n",
    "            n_tries = cfg.tta_count if cfg.use_tta else 1\n",
    "\n",
    "            for t in range(n_tries):\n",
    "                # preprocess + TTA\n",
    "                mel = process_audio_segment(seg, cfg)\n",
    "                if cfg.use_tta:\n",
    "                    mel = apply_tta(mel, t)\n",
    "                x = torch.tensor(mel, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(cfg.device)\n",
    "\n",
    "                # single ensemble forward\n",
    "                with torch.no_grad():\n",
    "                    logits = models(x)              # ensemble returns raw logits\n",
    "                    probs  = torch.sigmoid(logits)  # shape (1, C)\n",
    "                preds_per_try.append(probs.cpu().numpy().squeeze())\n",
    "\n",
    "            # average over TTA (or just take the one if no TTA)\n",
    "            final_preds = np.mean(preds_per_try, axis=0)\n",
    "            predictions.append(final_preds)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T06:46:02.288516Z",
     "iopub.status.busy": "2025-04-26T06:46:02.288138Z",
     "iopub.status.idle": "2025-04-26T06:46:02.303989Z",
     "shell.execute_reply": "2025-04-26T06:46:02.302823Z",
     "shell.execute_reply.started": "2025-04-26T06:46:02.288480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_tta(spec, tta_idx):\n",
    "    \"\"\"Apply test-time augmentation\"\"\"\n",
    "    if tta_idx == 0:\n",
    "        # Original spectrogram\n",
    "        return spec\n",
    "    elif tta_idx == 1:\n",
    "        # Time shift (horizontal flip)\n",
    "        return np.flip(spec, axis=1)\n",
    "    elif tta_idx == 2:\n",
    "        # Frequency shift (vertical flip)\n",
    "        return np.flip(spec, axis=0)\n",
    "    else:\n",
    "        return spec\n",
    "\n",
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for audio_path in tqdm(test_files):\n",
    "        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "        all_row_ids.extend(row_ids)\n",
    "        all_predictions.extend(predictions)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T06:46:02.305512Z",
     "iopub.status.busy": "2025-04-26T06:46:02.305142Z",
     "iopub.status.idle": "2025-04-26T06:46:02.326760Z",
     "shell.execute_reply": "2025-04-26T06:46:02.325550Z",
     "shell.execute_reply.started": "2025-04-26T06:46:02.305476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference…\")\n",
    "    print(f\"TTA enabled: {cfg.use_tta} \"\n",
    "          f\"(variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "\n",
    "    # load ONE ensemble module\n",
    "    ensemble = load_models(cfg, num_classes) \n",
    "    print(f\"Loaded {len(ensemble.models)} sub-models\")\n",
    "    print(\"Weights:\", ensemble.weights.cpu().numpy().flatten())\n",
    "    \n",
    "    # run inference ─────────────────────────────────────────\n",
    "    row_ids, predictions = run_inference(cfg, ensemble, species_ids)\n",
    "\n",
    "    # create Kaggle submission ─────────────────────────────\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "    submission_path = \"submission.csv\"\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "    elapsed = (time.time() - start_time) / 60\n",
    "    print(f\"Inference completed in {elapsed:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T06:46:02.328204Z",
     "iopub.status.busy": "2025-04-26T06:46:02.327844Z",
     "iopub.status.idle": "2025-04-26T06:46:03.129100Z",
     "shell.execute_reply": "2025-04-26T06:46:03.128034Z",
     "shell.execute_reply.started": "2025-04-26T06:46:02.328165Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference…\n",
      "TTA enabled: False (variations: 0)\n",
      "Loaded 2 sub-models\n",
      "Weights: [0.5 0.5]\n",
      "Found 0 test soundscapes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6e4060a9e84650b54a75d738bcdc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission dataframe...\n",
      "Submission saved to submission.csv\n",
      "Inference completed in 0.01 minutes\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "isSourceIdPinned": false,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 6891568,
     "sourceId": 11060723,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7240856,
     "sourceId": 11546330,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7254020,
     "sourceId": 11570442,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7254992,
     "sourceId": 11571998,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7255299,
     "sourceId": 11572420,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
