{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":11060723,"sourceType":"datasetVersion","datasetId":6891568},{"sourceId":11546330,"sourceType":"datasetVersion","datasetId":7240856},{"sourceId":11570442,"sourceType":"datasetVersion","datasetId":7254020},{"sourceId":11571998,"sourceType":"datasetVersion","datasetId":7254992},{"sourceId":11572420,"sourceType":"datasetVersion","datasetId":7255299}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **BirdCLEF 2025 Inference Notebook**\nThis notebook runs inference on BirdCLEF 2025 test soundscapes and generates a submission file. It supports both single model inference and ensemble inference with multiple models. You can find the pre-processing and training processes in the following notebooks:\n\n- [Transforming Audio-to-Mel Spec. | BirdCLEF'25](https://www.kaggle.com/code/kadircandrisolu/transforming-audio-to-mel-spec-birdclef-25)  \n- [EfficientNet B0 Pytorch [Train] | BirdCLEF'25](https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-train-birdclef-25)\n\n**Features**\n- Audio Preprocessing\n- Test-Time Augmentation (TTA)","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport warnings\nimport logging\nimport time\nimport math\nimport cv2\nfrom pathlib import Path\n\nimport copy                 \nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\nfrom tqdm.auto import tqdm\n\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.ERROR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T06:46:02.130791Z","iopub.execute_input":"2025-04-26T06:46:02.131180Z","iopub.status.idle":"2025-04-26T06:46:02.137324Z","shell.execute_reply.started":"2025-04-26T06:46:02.131151Z","shell.execute_reply":"2025-04-26T06:46:02.136015Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class CFG:\n \n    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n    model_path = '/kaggle/input/best-test'  \n    \n    # Audio & mel parameters\n    FS = 32_000; WINDOW_SIZE = 5\n    N_FFT = 1024; HOP_LENGTH = 512; N_MELS = 128\n    FMIN = 50; FMAX = 14_000\n    TARGET_SHAPE = (256, 256)\n\n    # Model / training options\n    in_channels   = 1\n    pretrained    = False          #‑‑ trained‑from‑scratch\n    dropout_rate  = 0.5\n    mixup_alpha   = 0.0\n\n    # Inference\n    device   = 'cpu'               # change to 'cuda' if available\n    batch_size = 16\n    use_tta    = False; tta_count = 3\n    threshold  = 0.5\n\n    # Debug\n    debug = False\n    debug_count = 3\n\ncfg = CFG()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T06:46:02.138569Z","iopub.execute_input":"2025-04-26T06:46:02.138850Z","iopub.status.idle":"2025-04-26T06:46:02.153596Z","shell.execute_reply.started":"2025-04-26T06:46:02.138825Z","shell.execute_reply":"2025-04-26T06:46:02.152500Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print(f\"Using device: {cfg.device}\")\nprint(f\"Loading taxonomy data...\")\ntaxonomy_df = pd.read_csv(cfg.taxonomy_csv)\nspecies_ids = taxonomy_df['primary_label'].tolist()\nnum_classes = len(species_ids)\nprint(f\"Number of classes: {num_classes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T06:46:02.155083Z","iopub.execute_input":"2025-04-26T06:46:02.155433Z","iopub.status.idle":"2025-04-26T06:46:02.182134Z","shell.execute_reply.started":"2025-04-26T06:46:02.155407Z","shell.execute_reply":"2025-04-26T06:46:02.180908Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nLoading taxonomy data...\nNumber of classes: 206\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"**Model 1 BiLSTM**","metadata":{}},{"cell_type":"code","source":"class GlobalAttentionPool(nn.Module):\n    \"\"\"\n    Learnable attention pooling:\n     - projects spatial features to K,V\n     - uses a single learnable query to attend over H×W tokens\n    \"\"\"\n    def __init__(self, input_dim, num_heads=8):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(input_dim, num_heads, batch_first=True)\n        self.query = nn.Parameter(torch.randn(1, 1, input_dim))\n        \n    def forward(self, x):\n        # x shape: [batch_size, seq_len, input_dim]\n        batch_size = x.size(0)\n        query = self.query.expand(batch_size, -1, -1)  # [batch_size, 1, input_dim]\n        \n        # Apply attention\n        attn_output, _ = self.attention(query, x, x)  # [batch_size, 1, input_dim]\n        \n        # Squeeze the sequence dimension\n        return attn_output.squeeze(1)  # [batch_size, input_dim]\nclass BiLSTM(nn.Module):\n    def __init__(self, cfg, num_classes=None):\n        super().__init__()\n        self.cfg = cfg\n    \n        # Use either passed num_classes or get from taxonomy\n        if num_classes is not None:\n            cfg.num_classes = num_classes\n        else:\n        # load label count from taxonomy file\n            taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n            cfg.num_classes = len(taxonomy_df)\n        \n        # Define input feature dimension\n        self.input_dim = cfg.input_dim  # Add this to your config\n        \n        # BiLSTM layers\n        self.lstm_hidden_size = getattr(cfg, 'lstm_hidden_size', 256)\n        self.lstm_num_layers = getattr(cfg, 'lstm_num_layers', 2)\n        self.lstm_dropout = getattr(cfg, 'lstm_dropout', 0.2)\n        \n        self.bilstm = nn.LSTM(\n            input_size=self.input_dim,\n            hidden_size=self.lstm_hidden_size,\n            num_layers=self.lstm_num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=self.lstm_dropout if self.lstm_num_layers > 1 else 0\n        )\n        \n        # Feature dimension after BiLSTM\n        bilstm_output_dim = self.lstm_hidden_size * 2  # *2 because bidirectional\n        self.feat_dim = bilstm_output_dim\n        \n        # Attention pooling\n        self.pool = GlobalAttentionPool(bilstm_output_dim, num_heads=8)\n        \n        # Projection head\n        hidden_dim = bilstm_output_dim // 2\n        self.proj_head = nn.Sequential(\n            nn.Linear(bilstm_output_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(p=getattr(cfg, 'dropout_rate', 0.5)),\n        )\n        \n        # Classifier\n        self.classifier = nn.Linear(hidden_dim, cfg.num_classes)\n        \n        # Mixup config\n        self.mixup_enabled = getattr(cfg, 'mixup_alpha', 0) > 0\n        if self.mixup_enabled:\n            self.mixup_alpha = cfg.mixup_alpha\n            \n    def forward(self, x, targets=None):\n        batch_size = x.size(0)\n        \n        # For BiLSTM, x should be [batch_size, sequence_length, features]\n        # Check if reshaping is needed based on input dimensions\n        if len(x.shape) == 4:  # [B, C, H, W] format (like a spectrogram)\n            # Reshape for LSTM: [batch_size, seq_len, features]\n            # Assuming x is [B, C, H, W], reshape to [B, H, W*C] or similar\n            # This depends on how your data is structured\n            x = x.permute(0, 2, 1, 3).contiguous()  # [B, H, C, W]\n            x = x.view(batch_size, x.size(1), -1)  # [B, H, C*W]\n        \n        # Apply BiLSTM\n        lstm_out, _ = self.bilstm(x)  # [B, seq_len, hidden_size*2]\n        \n        # Apply attention pooling\n        pooled = self.pool(lstm_out)  # [B, hidden_size*2]\n        \n        # Projection head\n        proj = self.proj_head(pooled)\n        \n        # Mixup logic if needed\n        if self.training and self.mixup_enabled and targets is not None:\n            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n            index = torch.randperm(batch_size).to(x.device)\n            \n            mixed_proj = lam * proj + (1 - lam) * proj[index, :]\n            logits = self.classifier(mixed_proj)\n            \n            return logits, lam, index\n        \n        # Standard forward pass\n        logits = self.classifier(proj)\n        \n        if targets is not None:\n            return logits, None, None\n        else:\n            return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T06:46:02.183730Z","iopub.execute_input":"2025-04-26T06:46:02.184223Z","iopub.status.idle":"2025-04-26T06:46:02.199193Z","shell.execute_reply.started":"2025-04-26T06:46:02.184180Z","shell.execute_reply":"2025-04-26T06:46:02.198147Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"**Model 2 Efficientnet**","metadata":{}},{"cell_type":"code","source":"# EfficientNet B0\nclass GlobalAttentionPool(nn.Module):\n    \"\"\"\n    Learnable attention pooling:\n     - projects spatial features to K,V\n     - uses a single learnable query to attend over H×W tokens\n    \"\"\"\n    def __init__(self, in_channels, num_heads=8):\n        super().__init__()\n        self.num_heads = num_heads\n        self.scale = (in_channels // num_heads) ** -0.5\n\n        # learnable query token: (1, 1, C)\n        self.query = nn.Parameter(torch.randn(1, 1, in_channels))\n\n        # projectors for keys & values\n        self.to_k = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n        self.to_v = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        # x: (B, C, H, W)\n        B, C, H, W = x.shape\n        # (B, C, H*W) → (B, H*W, C)\n        k = self.to_k(x).view(B, C, -1).permute(0, 2, 1)\n        v = self.to_v(x).view(B, C, -1).permute(0, 2, 1)\n        # expand query to batch\n        q = self.query.expand(B, -1, -1)                  # (B, 1, C)\n\n        # compute attention scores & aggregate\n        attn = (q @ k.transpose(-2, -1)) * self.scale     # (B, 1, H*W)\n        attn = attn.softmax(dim=-1)\n        out = attn @ v                                    # (B, 1, C)\n        return out.squeeze(1)                             # (B, C)\n\nclass BirdCLEFModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n\n        # load label count\n        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n        cfg.num_classes = len(taxonomy_df)\n\n        # backbone (e.g. EfficientNet, ResNet…)\n        self.backbone = timm.create_model(\n            cfg.model_name,\n            pretrained=cfg.pretrained,\n            in_chans=cfg.in_channels,\n            drop_rate=0.2,\n            drop_path_rate=0.2\n        )\n\n        # strip off original head\n        if 'efficientnet' in cfg.model_name:\n            feat_dim = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n        elif 'resnet' in cfg.model_name:\n            feat_dim = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n        else:\n            feat_dim = self.backbone.get_classifier().in_features\n            self.backbone.reset_classifier(0, '')\n\n        self.feat_dim = feat_dim\n\n        self.pool = GlobalAttentionPool(feat_dim, num_heads=8)\n\n        hidden_dim = feat_dim // 2\n        self.proj_head = nn.Sequential(\n            nn.Linear(feat_dim, hidden_dim),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(p=getattr(cfg, 'dropout_rate', 0.5)),\n        )\n\n        self.classifier = nn.Linear(hidden_dim, cfg.num_classes)\n\n        # mixup config\n        self.mixup_enabled = getattr(cfg, 'mixup_alpha', 0) > 0\n        if self.mixup_enabled:\n            self.mixup_alpha = cfg.mixup_alpha\n\n    def forward(self, x, targets=None):\n        if self.training and self.mixup_enabled and targets is not None:\n            x, ta, tb, lam = self.mixup_data(x, targets)\n        else:\n            ta, tb, lam = None, None, None\n\n        # extract features\n        feats = self.backbone(x)\n        if isinstance(feats, dict):\n            feats = feats['features']\n\n        # if 4D feature map → attention‐pool to 1D\n        if feats.ndim == 4:\n            feats = self.pool(feats)\n\n        # projection head\n        feats = self.proj_head(feats)\n\n        logits = self.classifier(feats)\n\n        # mixup‐aware loss\n        if self.training and self.mixup_enabled and targets is not None:\n            loss = self.mixup_criterion(F.binary_cross_entropy_with_logits,\n                                        logits, ta, tb, lam)\n            return logits, loss\n\n        return logits\n\n    def mixup_data(self, x, targets):\n        B = x.size(0)\n        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n        idx = torch.randperm(B, device=x.device)\n        mixed_x = lam * x + (1 - lam) * x[idx]\n        return mixed_x, targets, targets[idx], lam\n\n    def mixup_criterion(self, criterion, pred, y_a, y_b, lam):\n        return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T06:46:02.200508Z","iopub.execute_input":"2025-04-26T06:46:02.200921Z","iopub.status.idle":"2025-04-26T06:46:02.226175Z","shell.execute_reply.started":"2025-04-26T06:46:02.200884Z","shell.execute_reply":"2025-04-26T06:46:02.225093Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":" **Ensembel Model**","metadata":{}},{"cell_type":"code","source":"class BirdCLEFEnsembleModel(nn.Module):\n    def __init__(self, models, weights=None):\n        super().__init__()\n        self.models = nn.ModuleList(models)\n        n = len(models)\n        if weights is None:\n            weights = torch.ones(n) / n\n        self.register_buffer(\"weights\", torch.as_tensor(weights).view(n, 1, 1))\n\n    @torch.no_grad()\n    def forward(self, x):\n        outs = [torch.sigmoid(m(x)) for m in self.models]          # list of (B,C)\n        outs = torch.stack(outs)                                   # (n,B,C)\n        return (outs * self.weights).sum(0)                        # (B,C)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T06:46:02.227248Z","iopub.execute_input":"2025-04-26T06:46:02.227523Z","iopub.status.idle":"2025-04-26T06:46:02.250256Z","shell.execute_reply.started":"2025-04-26T06:46:02.227499Z","shell.execute_reply":"2025-04-26T06:46:02.249112Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"**Utilities**","metadata":{}},{"cell_type":"code","source":"def audio2melspec(audio_data, cfg):\n    \"\"\"Convert audio data to mel spectrogram\"\"\"\n    if np.isnan(audio_data).any():\n        mean_signal = np.nanmean(audio_data)\n        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n\n    mel_spec = librosa.feature.melspectrogram(\n        y=audio_data,\n        sr=cfg.FS,\n        n_fft=cfg.N_FFT,\n        hop_length=cfg.HOP_LENGTH,\n        n_mels=cfg.N_MELS,\n        fmin=cfg.FMIN,\n        fmax=cfg.FMAX,\n        power=2.0\n    )\n\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n    \n    return mel_spec_norm\n\ndef process_audio_segment(audio_data, cfg):\n    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n        audio_data = np.pad(audio_data, \n                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n                          mode='constant')\n    \n    mel_spec = audio2melspec(audio_data, cfg)\n    \n    # Resize if needed\n    if mel_spec.shape != cfg.TARGET_SHAPE:\n        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n        \n    return mel_spec.astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T06:46:02.252387Z","iopub.execute_input":"2025-04-26T06:46:02.252679Z","iopub.status.idle":"2025-04-26T06:46:02.271381Z","shell.execute_reply.started":"2025-04-26T06:46:02.252655Z","shell.execute_reply":"2025-04-26T06:46:02.269975Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def find_model_files(cfg):\n    \"\"\"\n    Find all .pth model files in the specified model directory\n    \"\"\"\n    model_files = []\n\n    model_dir = Path(cfg.model_path)\n\n    for path in model_dir.glob('**/*.pth'):\n        model_files.append(str(path))\n\n    return model_files\n    \ndef load_model_from_path(path, cfg, num_classes):\n    ckpt = torch.load(path, map_location=cfg.device)\n    path_lc = str(path).lower()\n    \n    # Add support for your specific model name pattern\n    if \"bilstm\" in path_lc:\n        tmp_cfg = copy.deepcopy(cfg)\n        tmp_cfg.num_classes = num_classes\n        model = BiLSTM(tmp_cfg)\n    elif \"efficientnet\" in path_lc:\n        tmp_cfg = copy.deepcopy(cfg)\n        tmp_cfg.model_name = \"efficientnet_b0\"\n        tmp_cfg.num_classes = num_classes\n        model = BirdCLEFModel(tmp_cfg)\n    elif \"fold\" in path_lc:  # Add specific check for \"fold\" in the filename\n        # Assuming the fold models are EfficientNet models\n        tmp_cfg = copy.deepcopy(cfg)\n        tmp_cfg.model_name = \"efficientnet_b0\"\n        tmp_cfg.num_classes = num_classes\n        model = BirdCLEFModel(tmp_cfg)\n\n    else:\n        raise ValueError(f\"Cannot infer backbone type from {path}\")\n    \n    # load weights (strict=False so missing optimizer keys won't error)\n    model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n    return model.to(cfg.device).eval()\n\n\ndef load_models(cfg, num_classes):\n    paths  = find_model_files(cfg)\n    models = [load_model_from_path(p, cfg, num_classes) for p in paths]\n    if not models:\n        raise RuntimeError(\"No .pth files found in model_path\")\n    return BirdCLEFEnsembleModel(models)      # <- single nn.Module\n\n\n\ndef predict_on_spectrogram(audio_path, models, cfg, species_ids):\n    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n    predictions = []\n    row_ids     = []\n    soundscape_id = Path(audio_path).stem\n\n    try:\n        print(f\"Processing {soundscape_id}\")\n        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n\n        for segment_idx in range(total_segments):\n            start = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n            end   = start + cfg.FS * cfg.WINDOW_SIZE\n            seg   = audio_data[start:end]\n\n            # build row_id\n            t_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n            row_ids.append(f\"{soundscape_id}_{t_sec}\")\n\n            # collect preds (with or without TTA)\n            preds_per_try = []\n            n_tries = cfg.tta_count if cfg.use_tta else 1\n\n            for t in range(n_tries):\n                # preprocess + TTA\n                mel = process_audio_segment(seg, cfg)\n                if cfg.use_tta:\n                    mel = apply_tta(mel, t)\n                x = torch.tensor(mel, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(cfg.device)\n\n                # single ensemble forward\n                with torch.no_grad():\n                    logits = models(x)              # ensemble returns raw logits\n                    probs  = torch.sigmoid(logits)  # shape (1, C)\n                preds_per_try.append(probs.cpu().numpy().squeeze())\n\n            # average over TTA (or just take the one if no TTA)\n            final_preds = np.mean(preds_per_try, axis=0)\n            predictions.append(final_preds)\n\n    except Exception as e:\n        print(f\"Error processing {audio_path}: {e}\")\n\n    return row_ids, predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T06:46:02.272683Z","iopub.execute_input":"2025-04-26T06:46:02.272959Z","iopub.status.idle":"2025-04-26T06:46:02.287188Z","shell.execute_reply.started":"2025-04-26T06:46:02.272935Z","shell.execute_reply":"2025-04-26T06:46:02.286048Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def apply_tta(spec, tta_idx):\n    \"\"\"Apply test-time augmentation\"\"\"\n    if tta_idx == 0:\n        # Original spectrogram\n        return spec\n    elif tta_idx == 1:\n        # Time shift (horizontal flip)\n        return np.flip(spec, axis=1)\n    elif tta_idx == 2:\n        # Frequency shift (vertical flip)\n        return np.flip(spec, axis=0)\n    else:\n        return spec\n\ndef run_inference(cfg, models, species_ids):\n    \"\"\"Run inference on all test soundscapes\"\"\"\n    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n    \n    if cfg.debug:\n        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n        test_files = test_files[:cfg.debug_count]\n    \n    print(f\"Found {len(test_files)} test soundscapes\")\n\n    all_row_ids = []\n    all_predictions = []\n\n    for audio_path in tqdm(test_files):\n        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n        all_row_ids.extend(row_ids)\n        all_predictions.extend(predictions)\n    \n    return all_row_ids, all_predictions\n\ndef create_submission(row_ids, predictions, species_ids, cfg):\n    \"\"\"Create submission dataframe\"\"\"\n    print(\"Creating submission dataframe...\")\n\n    submission_dict = {'row_id': row_ids}\n    \n    for i, species in enumerate(species_ids):\n        submission_dict[species] = [pred[i] for pred in predictions]\n\n    submission_df = pd.DataFrame(submission_dict)\n\n    submission_df.set_index('row_id', inplace=True)\n\n    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n\n    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n    if missing_cols:\n        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n        for col in missing_cols:\n            submission_df[col] = 0.0\n\n    submission_df = submission_df[sample_sub.columns]\n\n    submission_df = submission_df.reset_index()\n    \n    return submission_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T06:46:02.288138Z","iopub.execute_input":"2025-04-26T06:46:02.288516Z","iopub.status.idle":"2025-04-26T06:46:02.303989Z","shell.execute_reply.started":"2025-04-26T06:46:02.288480Z","shell.execute_reply":"2025-04-26T06:46:02.302823Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def main():\n    start_time = time.time()\n    print(\"Starting BirdCLEF-2025 inference…\")\n    print(f\"TTA enabled: {cfg.use_tta} \"\n          f\"(variations: {cfg.tta_count if cfg.use_tta else 0})\")\n\n    # load ONE ensemble module\n    ensemble = load_models(cfg, num_classes) \n    print(f\"Loaded {len(ensemble.models)} sub-models\")\n    print(\"Weights:\", ensemble.weights.cpu().numpy().flatten())\n    \n    # run inference ─────────────────────────────────────────\n    row_ids, predictions = run_inference(cfg, ensemble, species_ids)\n\n    # create Kaggle submission ─────────────────────────────\n    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n    submission_path = \"submission.csv\"\n    submission_df.to_csv(submission_path, index=False)\n    print(f\"Submission saved to {submission_path}\")\n\n    elapsed = (time.time() - start_time) / 60\n    print(f\"Inference completed in {elapsed:.2f} minutes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T06:46:02.305142Z","iopub.execute_input":"2025-04-26T06:46:02.305512Z","iopub.status.idle":"2025-04-26T06:46:02.326760Z","shell.execute_reply.started":"2025-04-26T06:46:02.305476Z","shell.execute_reply":"2025-04-26T06:46:02.325550Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T06:46:02.327844Z","iopub.execute_input":"2025-04-26T06:46:02.328204Z","iopub.status.idle":"2025-04-26T06:46:03.129100Z","shell.execute_reply.started":"2025-04-26T06:46:02.328165Z","shell.execute_reply":"2025-04-26T06:46:03.128034Z"}},"outputs":[{"name":"stdout","text":"Starting BirdCLEF-2025 inference…\nTTA enabled: False (variations: 0)\nLoaded 2 sub-models\nWeights: [0.5 0.5]\nFound 0 test soundscapes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da6e4060a9e84650b54a75d738bcdc78"}},"metadata":{}},{"name":"stdout","text":"Creating submission dataframe...\nSubmission saved to submission.csv\nInference completed in 0.01 minutes\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}